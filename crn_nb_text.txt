{"cells": [{"cell_type": "markdown", "id": "md00", "metadata": {"trusted": true}, "source": "# CRN Baseline — STFT Speech Enhancement (Fixed)\n\n**What changed from the original CRN baseline:**\n1. Switched from **Mel spectrogram** (non-invertible) → **STFT** (lossless ISTFT)\n2. Real **PESQ / STOI / SI-SDR** on actual waveforms (no more \"estimated\" metrics)\n3. Proper CRN architecture: CNN encoder → LSTM (per-frequency-bin) → CNN decoder → sigmoid mask\n4. Checkpoint saving works correctly\n\n**Architecture:**\n```\nInput: (B, 1, 257, T) ← log1p(STFT magnitude)\n  → CNN Encoder (3 layers, stride-2 on freq): (B, 256, 33, T)\n  → Reshape → LSTM(256, hidden=256, 2 layers) across time\n  → CNN Decoder + Sigmoid → mask (B, 257, T)\nReconstruction: mask × noisy_mag → ISTFT with noisy phase → waveform\n```\n\n**Team:** Krishnasinh Jadeja (22BLC1211), Kirtan Sondagar (22BLC1228), Prabhu Kalyan Panda (22BLC1213)\n**Guide:** Dr. Praveen Jaraut — VIT Bhopal Capstone"}, {"cell_type": "code", "execution_count": null, "id": "c01", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 1: Install deps + Imports + Config\n# ============================================================================\n!pip install pesq==0.0.4 pystoi -q\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport glob, os, json, time, warnings\nwarnings.filterwarnings('ignore')\nfrom pesq import pesq as pesq_metric\nfrom pystoi import stoi as stoi_metric\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# STFT config (same as all reviews — consistent comparison)\nN_FFT      = 512\nHOP_LENGTH = 256\nN_FREQ     = N_FFT // 2 + 1   # 257\nSR         = 16000\nMAX_LEN    = 48000             # 3 s\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\nif device == 'cuda':\n    props = torch.cuda.get_device_properties(0)\n    vram = getattr(props, 'total_memory', getattr(props, 'total_mem', 0))\n    print(f'GPU: {torch.cuda.get_device_name(0)} | VRAM: {vram/1e9:.1f}GB')\nprint(f'STFT: n_fft={N_FFT}, hop={HOP_LENGTH}, freq={N_FREQ}')\nprint('Imports OK')"}, {"cell_type": "markdown", "id": "md02", "metadata": {"trusted": true}, "source": "## Dataset: LibriSpeech-Noise\nDownload & extract `earth16/libri-speech-noise-dataset` (7000 train + 105 test WAV pairs)."}, {"cell_type": "code", "execution_count": null, "id": "c03", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 2: Dataset download & extraction\n# ============================================================================\nimport subprocess, zipfile\n\ndata_base = '/kaggle/working/data'\ndl_tmp    = '/kaggle/working/dl_tmp'\nos.makedirs(data_base, exist_ok=True)\nos.makedirs(dl_tmp, exist_ok=True)\ndone_flag = os.path.join(data_base, '.done')\n\nif os.path.exists(done_flag):\n    print('Dataset already extracted, skipping')\nelse:\n    mounted = '/kaggle/input/libri-speech-noise-dataset'\n    if os.path.isdir(mounted) and len(os.listdir(mounted)) > 0:\n        src = mounted\n        print(f'Using mounted dataset at {src}')\n    else:\n        print('Dataset not mounted, downloading via kaggle API...')\n        subprocess.run(['kaggle', 'datasets', 'download',\n                        'earth16/libri-speech-noise-dataset', '-p', dl_tmp], check=True)\n        zf = os.path.join(dl_tmp, 'libri-speech-noise-dataset.zip')\n        if os.path.exists(zf):\n            with zipfile.ZipFile(zf, 'r') as z:\n                z.extractall(dl_tmp)\n            os.remove(zf)\n        src = dl_tmp\n        print(f'Downloaded to {src}')\n\n    subprocess.run(['apt-get', 'install', '-y', 'p7zip-full'], capture_output=True)\n    for arch in ['train.7z', 'y_train.7z', 'test.7z', 'y_test.7z']:\n        fp = os.path.join(src, arch)\n        if os.path.exists(fp):\n            print(f'Extracting {arch}...')\n            subprocess.run(['7z', 'x', fp, f'-o{data_base}', '-y'], capture_output=True)\n    open(done_flag, 'w').close()\n\ndef find_wav_dir(base, name):\n    for root, dirs, files in os.walk(base):\n        if os.path.basename(root) == name and any(f.endswith('.wav') for f in files):\n            return root\n    return None\n\nnoisy_train = find_wav_dir(data_base, 'train')\nclean_train = find_wav_dir(data_base, 'y_train')\nnoisy_test  = find_wav_dir(data_base, 'test')\nclean_test  = find_wav_dir(data_base, 'y_test')\n\nfor tag, d in [('noisy_train', noisy_train), ('clean_train', clean_train),\n               ('noisy_test', noisy_test), ('clean_test', clean_test)]:\n    n = len(glob.glob(os.path.join(d, '*.wav'))) if d else 0\n    print(f'  {tag}: {d} ({n} files)')"}, {"cell_type": "markdown", "id": "md04", "metadata": {"trusted": true}, "source": "## STFT Dataset\nSame STFT config as R2/R3 for fair comparison. Returns magnitude, phase, waveforms."}, {"cell_type": "code", "execution_count": null, "id": "c05", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 3: STFTSpeechDataset\n# ============================================================================\nclass STFTSpeechDataset(Dataset):\n    def __init__(self, noisy_dir, clean_dir, n_fft=N_FFT, hop_length=HOP_LENGTH,\n                 sr=SR, max_len=MAX_LEN):\n        self.noisy_files = sorted(glob.glob(os.path.join(noisy_dir, '*.wav')))\n        self.clean_files = sorted(glob.glob(os.path.join(clean_dir, '*.wav')))\n        assert len(self.noisy_files) == len(self.clean_files), \\\n            f'Mismatch: {len(self.noisy_files)} noisy vs {len(self.clean_files)} clean'\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.sr = sr\n        self.max_len = max_len\n        self.window = torch.hann_window(n_fft)\n\n    def __len__(self):\n        return len(self.noisy_files)\n\n    def _load_fix(self, path):\n        wav, sr = torchaudio.load(path)\n        if sr != self.sr:\n            wav = torchaudio.functional.resample(wav, sr, self.sr)\n        wav = wav[0]\n        if wav.shape[0] > self.max_len:\n            start = torch.randint(0, wav.shape[0] - self.max_len, (1,)).item()\n            wav = wav[start:start + self.max_len]\n        elif wav.shape[0] < self.max_len:\n            wav = F.pad(wav, (0, self.max_len - wav.shape[0]))\n        return wav\n\n    def __getitem__(self, idx):\n        noisy_wav = self._load_fix(self.noisy_files[idx])\n        clean_wav = self._load_fix(self.clean_files[idx])\n        noisy_stft = torch.stft(noisy_wav, self.n_fft, self.hop_length,\n                                window=self.window, return_complex=True)\n        clean_stft = torch.stft(clean_wav, self.n_fft, self.hop_length,\n                                window=self.window, return_complex=True)\n        return {\n            'noisy_mag':   noisy_stft.abs(),\n            'clean_mag':   clean_stft.abs(),\n            'noisy_phase': torch.angle(noisy_stft),\n            'noisy_wav':   noisy_wav,\n            'clean_wav':   clean_wav,\n        }\n\nprint(f'STFTSpeechDataset defined (n_fft={N_FFT}, hop={HOP_LENGTH})')"}, {"cell_type": "markdown", "id": "md06", "metadata": {"trusted": true}, "source": "## CRN Model (Fixed)\n\n**Key fixes from original:**\n1. **STFT input** (257 freq bins) instead of mel (128) — enables lossless ISTFT reconstruction\n2. **Per-frequency LSTM**: reshapes to `(B*F', T, C)` so LSTM models temporal dynamics for each frequency sub-band independently — instead of collapsing frequency with `mean(dim=2)`\n3. **Proper CNN freq downsampling/upsampling** with stride-2 convolutions + transposed convolutions\n\n```\nCNN Encoder: (B, 1, 257, T)\n  -> Conv2d(1->64, stride=(2,1)) -> (B, 64, 129, T)\n  -> Conv2d(64->128, stride=(2,1)) -> (B, 128, 65, T)\n  -> Conv2d(128->256, stride=(2,1)) -> (B, 256, 33, T)\nLSTM: reshape to (B*33, T, 256) -> LSTM(256, 256, 2 layers) -> (B*33, T, 256)\nCNN Decoder: (B, 256, 33, T)\n  -> ConvT2d(256->128, stride=(2,1)) -> (B, 128, 65, T)\n  -> ConvT2d(128->64, stride=(2,1)) -> (B, 64, 129, T)\n  -> ConvT2d(64->32, stride=(2,1)) -> (B, 32, 257, T)\n  -> Conv2d(32->1, 1x1) + Sigmoid -> mask (B, 1, 257, T)\n```"}, {"cell_type": "code", "execution_count": null, "id": "c07", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 4: CRN Baseline Model (Fixed -- STFT based)\n# ============================================================================\nclass CRNBaseline(nn.Module):\n    \"\"\"CRN for STFT-based speech enhancement.\"\"\"\n    def __init__(self, n_freq=257):\n        super().__init__()\n        self.n_freq = n_freq\n\n        # CNN Encoder: downsample frequency with stride-2\n        self.enc1 = nn.Sequential(\n            nn.Conv2d(1, 64, kernel_size=3, stride=(2, 1), padding=1),\n            nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n        self.enc2 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=3, stride=(2, 1), padding=1),\n            nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n        self.enc3 = nn.Sequential(\n            nn.Conv2d(128, 256, kernel_size=3, stride=(2, 1), padding=1),\n            nn.BatchNorm2d(256), nn.ReLU(inplace=True))\n\n        # LSTM: processes each frequency sub-band across time\n        self.lstm = nn.LSTM(\n            input_size=256, hidden_size=256, num_layers=2,\n            batch_first=True, dropout=0.1)\n\n        # CNN Decoder: upsample frequency back with transposed convolutions\n        self.dec3 = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=3, stride=(2, 1), padding=1, output_padding=(1, 0)),\n            nn.BatchNorm2d(128), nn.ReLU(inplace=True))\n        self.dec2 = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, kernel_size=3, stride=(2, 1), padding=1, output_padding=(1, 0)),\n            nn.BatchNorm2d(64), nn.ReLU(inplace=True))\n        self.dec1 = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=3, stride=(2, 1), padding=1, output_padding=(1, 0)),\n            nn.BatchNorm2d(32), nn.ReLU(inplace=True))\n\n        # Final 1x1 conv + sigmoid mask\n        self.mask_conv = nn.Sequential(\n            nn.Conv2d(32, 1, kernel_size=1),\n            nn.Sigmoid())\n\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.Linear)):\n                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight)\n                nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.LSTM):\n                for name, param in m.named_parameters():\n                    if 'weight' in name:\n                        nn.init.xavier_normal_(param)\n                    elif 'bias' in name:\n                        nn.init.zeros_(param)\n\n    def forward(self, x):\n        # x: (B, 1, n_freq, T)\n        B, _, F_orig, T_orig = x.shape\n\n        # Encode (downsample freq: 257->129->65->33)\n        e1 = self.enc1(x)     # (B, 64, 129, T)\n        e2 = self.enc2(e1)    # (B, 128, 65, T)\n        e3 = self.enc3(e2)    # (B, 256, 33, T)\n\n        # LSTM: per-frequency-bin processing across time\n        B2, C, Fenc, T = e3.shape\n        # Reshape: (B, C, Fenc, T) -> (B*Fenc, T, C)\n        lstm_in = e3.permute(0, 2, 3, 1).reshape(B2 * Fenc, T, C)\n        lstm_out, _ = self.lstm(lstm_in)  # (B*Fenc, T, C)\n        # Reshape back: (B*Fenc, T, C) -> (B, C, Fenc, T)\n        h = lstm_out.reshape(B2, Fenc, T, C).permute(0, 3, 1, 2)  # (B, 256, 33, T)\n\n        # Decode (upsample freq: 33->65->129->257)\n        d3 = self.dec3(h)      # (B, 128, 65, T)\n        d2 = self.dec2(d3)     # (B, 64, 129, T)\n        d1 = self.dec1(d2)     # (B, 32, 257, T)\n\n        # Crop/pad to match original freq dimension\n        if d1.shape[2] != F_orig:\n            d1 = F.interpolate(d1, size=(F_orig, T_orig), mode='bilinear', align_corners=False)\n\n        mask = self.mask_conv(d1).squeeze(1)  # (B, n_freq, T)\n        return mask\n\n\n# Quick test\nmodel = CRNBaseline(n_freq=N_FREQ).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'CRNBaseline: {total_params:,} params ({total_params/1e6:.2f}M)')\n\nwith torch.no_grad():\n    dummy = torch.randn(2, 1, N_FREQ, 188).to(device)\n    mask = model(dummy)\n    print(f'Input: {dummy.shape} -> Mask: {mask.shape}')\n    assert mask.shape == (2, N_FREQ, 188), f'Shape mismatch: {mask.shape}'\n    assert mask.min().item() >= 0 and mask.max().item() <= 1\n    print(f'Mask range: [{mask.min().item():.4f}, {mask.max().item():.4f}]')\n    print('Forward pass OK')\n\n# Architecture breakdown\nenc_p = sum(p.numel() for n, p in model.named_parameters() if 'enc' in n)\nlstm_p = sum(p.numel() for n, p in model.named_parameters() if 'lstm' in n)\ndec_p = sum(p.numel() for n, p in model.named_parameters() if 'dec' in n or 'mask_conv' in n)\nprint(f'  Encoder:  {enc_p:>10,} ({enc_p/total_params*100:.1f}%)')\nprint(f'  LSTM:     {lstm_p:>10,} ({lstm_p/total_params*100:.1f}%)')\nprint(f'  Decoder:  {dec_p:>10,} ({dec_p/total_params*100:.1f}%)')"}, {"cell_type": "code", "execution_count": null, "id": "c08", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 5: SI-SDR utility\n# ============================================================================\ndef si_sdr(estimate, reference):\n    ref = reference - reference.mean()\n    est = estimate  - estimate.mean()\n    dot = torch.sum(ref * est)\n    s_target = dot * ref / (torch.sum(ref**2) + 1e-8)\n    e_noise  = est - s_target\n    return 10 * torch.log10(torch.sum(s_target**2) / (torch.sum(e_noise**2) + 1e-8) + 1e-8)\n\nprint('si_sdr defined')"}, {"cell_type": "markdown", "id": "md09", "metadata": {"trusted": true}, "source": "## Training\nL1 loss on log-magnitude, Adam lr=1e-3 with ReduceLROnPlateau.\nBatch=16, 25 epochs, patience=10."}, {"cell_type": "code", "execution_count": null, "id": "c10", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 6: Training setup\n# ============================================================================\nMAX_EPOCHS    = 25\nLR            = 1e-3\nBATCH         = 16\nPATIENCE      = 10\nCKPT          = 'crn_baseline_best.pth'\n\n# Re-init model fresh\nmodel = CRNBaseline(n_freq=N_FREQ).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Params: {total_params:,}')\n\n# Datasets\nfull_train = STFTSpeechDataset(noisy_train, clean_train)\nn_val   = int(0.1 * len(full_train))\nn_train = len(full_train) - n_val\ntrain_ds, val_ds = torch.utils.data.random_split(\n    full_train, [n_train, n_val], generator=torch.Generator().manual_seed(42))\ntest_ds = STFTSpeechDataset(noisy_test, clean_test)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n                          num_workers=0, drop_last=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, 'min', factor=0.5, patience=5)\n\nprint(f'Train:{n_train} Val:{n_val} Test:{len(test_ds)} | BS={BATCH} LR={LR}')"}, {"cell_type": "code", "execution_count": null, "id": "c11", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 7: Training loop\n# ============================================================================\nhistory = {'train_loss': [], 'val_loss': []}\nbest_val = float('inf')\npatience_ctr = 0\nt0 = time.time()\n\nfor epoch in range(1, MAX_EPOCHS + 1):\n    # --- Train ---\n    model.train()\n    train_losses = []\n    for batch in tqdm(train_loader, desc=f'Ep{epoch}/{MAX_EPOCHS}', leave=False):\n        noisy_mag = batch['noisy_mag'].to(device)\n        clean_mag = batch['clean_mag'].to(device)\n        inp  = torch.log1p(noisy_mag).unsqueeze(1)   # (B, 1, 257, T)\n        mask = model(inp)                              # (B, 257, T)\n        enhanced_mag = mask * noisy_mag\n        loss = F.l1_loss(torch.log1p(enhanced_mag), torch.log1p(clean_mag))\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # --- Validate ---\n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            noisy_mag = batch['noisy_mag'].to(device)\n            clean_mag = batch['clean_mag'].to(device)\n            inp  = torch.log1p(noisy_mag).unsqueeze(1)\n            mask = model(inp)\n            enhanced_mag = mask * noisy_mag\n            loss = F.l1_loss(torch.log1p(enhanced_mag), torch.log1p(clean_mag))\n            val_losses.append(loss.item())\n\n    tr_loss = np.mean(train_losses)\n    va_loss = np.mean(val_losses)\n    history['train_loss'].append(tr_loss)\n    history['val_loss'].append(va_loss)\n    scheduler.step(va_loss)\n\n    elapsed = time.time() - t0\n    lr_now  = optimizer.param_groups[0]['lr']\n    line = f'Ep{epoch:02d} tr={tr_loss:.4f} va={va_loss:.4f} lr={lr_now:.1e} [{elapsed:.0f}s]'\n\n    if va_loss < best_val:\n        best_val = va_loss\n        patience_ctr = 0\n        torch.save({'epoch': epoch, 'model': model.state_dict(),\n                     'val_loss': float(va_loss)}, CKPT)\n        print(f'{line}  SAVED best={va_loss:.4f}')\n    else:\n        patience_ctr += 1\n        print(f'{line}  no improve ({patience_ctr}/{PATIENCE})')\n\n    if epoch % 5 == 0:\n        torch.save(model.state_dict(), f'ckpt_ep{epoch}.pth')\n\n    if patience_ctr >= PATIENCE:\n        print(f'Early stopping at epoch {epoch}')\n        break\n\nbest_ep = history['val_loss'].index(min(history['val_loss'])) + 1\nprint(f'\\nDONE best_ep={best_ep} best_val={best_val:.4f} time={time.time()-t0:.0f}s')"}, {"cell_type": "markdown", "id": "md12", "metadata": {"trusted": true}, "source": "## Results"}, {"cell_type": "code", "execution_count": null, "id": "c13", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 8: Training curves\n# ============================================================================\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\neps = range(1, len(history['train_loss']) + 1)\nax.plot(eps, history['train_loss'], 'b-o', label='Train', ms=3)\nax.plot(eps, history['val_loss'], 'r-s', label='Val', ms=3)\nax.axvline(best_ep, color='g', ls='--', alpha=0.7, label=f'Best (ep{best_ep})')\nax.set_xlabel('Epoch')\nax.set_ylabel('L1 Loss (log-magnitude)')\nax.set_title('CRN Baseline (Fixed STFT) — Training Curves')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('training_curves.png', dpi=150)\nplt.show()\nprint('Saved training_curves.png')"}, {"cell_type": "markdown", "id": "md14", "metadata": {"trusted": true}, "source": "## Evaluation\n**REAL** PESQ / STOI / SI-SDR on 105 test samples via ISTFT waveform reconstruction.\nNo more estimated metrics!"}, {"cell_type": "code", "execution_count": null, "id": "c15", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 9: Evaluation — REAL PESQ / STOI / SI-SDR\n# ============================================================================\nckpt = torch.load(CKPT, map_location=device, weights_only=False)\nmodel.load_state_dict(ckpt['model'])\nmodel.eval()\nprint(f'Loaded: epoch={ckpt[\"epoch\"]}, val_loss={ckpt[\"val_loss\"]:.4f}')\n\nwindow_eval = torch.hann_window(N_FFT).to(device)\n\npesq_noisy_list, pesq_enh_list   = [], []\nstoi_noisy_list, stoi_enh_list   = [], []\nsisdr_noisy_list, sisdr_enh_list = [], []\n\nfor i in tqdm(range(len(test_ds)), desc='Eval'):\n    s           = test_ds[i]\n    noisy_mag   = s['noisy_mag'].unsqueeze(0).to(device)\n    noisy_phase = s['noisy_phase'].unsqueeze(0).to(device)\n    clean_np    = s['clean_wav'].numpy()\n    noisy_np    = s['noisy_wav'].numpy()\n\n    with torch.no_grad():\n        inp     = torch.log1p(noisy_mag).unsqueeze(1)\n        mask    = model(inp)\n        enh_mag = (mask * noisy_mag).squeeze(0)\n\n    enh_stft = enh_mag * torch.exp(1j * noisy_phase.squeeze(0))\n    enh_wav  = torch.istft(enh_stft, N_FFT, HOP_LENGTH,\n                           window=window_eval, length=MAX_LEN)\n    enh_np   = enh_wav.cpu().numpy()\n\n    try:\n        pesq_noisy_list.append(pesq_metric(SR, clean_np, noisy_np, 'wb'))\n        pesq_enh_list.append(  pesq_metric(SR, clean_np, enh_np,   'wb'))\n    except Exception as e:\n        print(f'  PESQ err {i}: {e}')\n\n    stoi_noisy_list.append(stoi_metric(clean_np, noisy_np, SR, extended=False))\n    stoi_enh_list.append(  stoi_metric(clean_np, enh_np,   SR, extended=False))\n\n    c_t = torch.from_numpy(clean_np).float()\n    n_t = torch.from_numpy(noisy_np).float()\n    e_t = torch.from_numpy(enh_np).float()\n    sisdr_noisy_list.append(si_sdr(n_t, c_t).item())\n    sisdr_enh_list.append(  si_sdr(e_t, c_t).item())\n\navg = lambda lst: float(np.mean(lst)) if lst else 0.0\navg_pesq_n,  avg_pesq_e  = avg(pesq_noisy_list),  avg(pesq_enh_list)\navg_stoi_n,  avg_stoi_e  = avg(stoi_noisy_list),  avg(stoi_enh_list)\navg_sisdr_n, avg_sisdr_e = avg(sisdr_noisy_list), avg(sisdr_enh_list)\n\nprint(f'\\nResults on {len(test_ds)} test files:')\nprint(f'  PESQ  : noisy={avg_pesq_n:.3f}  enhanced={avg_pesq_e:.3f}  d={avg_pesq_e-avg_pesq_n:+.3f}')\nprint(f'  STOI  : noisy={avg_stoi_n:.3f}  enhanced={avg_stoi_e:.3f}  d={avg_stoi_e-avg_stoi_n:+.4f}')\nprint(f'  SI-SDR: noisy={avg_sisdr_n:.2f}dB  enh={avg_sisdr_e:.2f}dB  d={avg_sisdr_e-avg_sisdr_n:+.2f}dB')"}, {"cell_type": "markdown", "id": "md16", "metadata": {"trusted": true}, "source": "## Visualization\nSpectrogram comparison: noisy vs enhanced vs clean, plus predicted mask."}, {"cell_type": "code", "execution_count": null, "id": "c17", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 10: Spectrogram comparison\n# ============================================================================\nsample = test_ds[0]\nnoisy_mag_s = sample['noisy_mag'].unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    inp_s  = torch.log1p(noisy_mag_s).unsqueeze(1)\n    mask_s = model(inp_s)\n    enh_mag_s = (mask_s * noisy_mag_s).squeeze(0).cpu()\n\nnoisy_spec = sample['noisy_mag'].numpy()\nclean_spec = sample['clean_mag'].numpy()\nenh_spec   = enh_mag_s.numpy()\nmask_np    = mask_s.squeeze(0).cpu().numpy()\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\nfor ax, spec, title in [\n    (axes[0,0], np.log1p(noisy_spec), 'Noisy Input'),\n    (axes[0,1], np.log1p(clean_spec), 'Clean Target'),\n    (axes[1,0], np.log1p(enh_spec),   'Enhanced (CRN)'),\n    (axes[1,1], mask_np,              'Predicted Mask'),\n]:\n    im = ax.imshow(spec, aspect='auto', origin='lower', cmap='viridis')\n    ax.set_title(title, fontsize=13)\n    ax.set_xlabel('Time frame')\n    ax.set_ylabel('Frequency bin')\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\nplt.suptitle('CRN Baseline: Spectrogram Comparison (Test Sample 0)', fontsize=14)\nplt.tight_layout()\nplt.savefig('spectrogram_comparison.png', dpi=150)\nplt.show()\nprint('Saved spectrogram_comparison.png')"}, {"cell_type": "code", "execution_count": null, "id": "c18", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 11: Summary JSON + comparison table\n# ============================================================================\nsummary = {\n    'review': 'R1_CRN_Fixed',\n    'model': 'CRNBaseline',\n    'approach': 'Conv-Recurrent Network (STFT-based)',\n    'pipeline': 'STFT (n_fft=512, hop=256) -> CRN mask -> ISTFT',\n    'params': total_params,\n    'checkpoint': {'epoch': int(ckpt['epoch']), 'val_loss': float(ckpt['val_loss'])},\n    'test_samples': len(test_ds),\n    'metrics': {\n        'pesq_noisy':        round(avg_pesq_n, 3),\n        'pesq_enhanced':     round(avg_pesq_e, 3),\n        'stoi_noisy':        round(avg_stoi_n, 4),\n        'stoi_enhanced':     round(avg_stoi_e, 4),\n        'sisdr_noisy_dB':    round(avg_sisdr_n, 2),\n        'sisdr_enhanced_dB': round(avg_sisdr_e, 2),\n    },\n    'history': history,\n}\nwith open('crn_baseline_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\nprint('Saved crn_baseline_summary.json')\n\nW = 80\nprint('\\n' + '='*W)\nprint(f'{\"Model\":>15} {\"Pipeline\":>20} {\"PESQ\":>8} {\"STOI\":>7} {\"SI-SDR\":>10} {\"Params\":>10}')\nprint('='*W)\nprint(f'{\"Noisy\":>15} {\"---\":>20} {avg_pesq_n:>8.3f} {avg_stoi_n:>7.3f} {avg_sisdr_n:>9.2f}dB {\"---\":>10}')\nprint(f'{\"CRN (fixed)\":>15} {\"LSTM+STFT\":>20} {avg_pesq_e:>8.3f} {avg_stoi_e:>7.3f} {avg_sisdr_e:>9.2f}dB {total_params/1e6:>8.2f}M')\nprint('='*W)\ndp = avg_pesq_e - avg_pesq_n\nds = avg_stoi_e - avg_stoi_n\ndd = avg_sisdr_e - avg_sisdr_n\nprint(f'CRN vs noisy: dPESQ={dp:+.3f}  dSTOI={ds:+.4f}  dSI-SDR={dd:+.2f}dB')"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 5}