{"cells": [{"cell_type": "markdown", "id": "md00", "metadata": {"trusted": true}, "source": "# Review 3: Dual-Path Transformer v2 — Aligned-Crop STFT Speech Enhancement\n\n## Critical Bug Fix from v1\n**v1 had a fatal data-loading bug:** `_load_fix()` was called independently for noisy and clean files —\neach call generated its OWN random crop position.  Since WAV files are 16–24 seconds long and we crop\n3-second segments, the noisy and clean waveforms came from **completely different time positions** in the\nutterance.  The model was training on mismatched (noisy segment A, clean segment B) pairs.\n\n**Evidence:** v1 noisy baseline had STOI = 0.218 and SI-SDR = −43.34 dB — indicating the \"pairs\" were\nessentially unrelated audio.\n\n**Fix:** `__getitem__` now computes **one** random start position and applies it to **both** files.\nFor test evaluation, `start = 0` (deterministic) so metrics are fully reproducible.\n\n**Architecture:** Dual-Path Transformer (DPT) — \"Lightweight Speech Enhancement Using Shallow Transformers\"\n```\nCNN Encoder (stride-2 on freq): (B, 1, 257, T) → (B, 128, 65, T)\n  → DualPathBlock ×2:\n      FreqTransformer: attend across 65 freq bins per time step\n      TimeTransformer: attend across T time steps per freq bin\n  → Skip + Interpolate → (B, 128, 257, T)\n  → CNN Decoder + Sigmoid → mask (B, 257, T)\nReconstruction: mask × noisy_mag → ISTFT with noisy phase → waveform\n```\n\n**Team:** Krishnasinh Jadeja (22BLC1211), Kirtan Sondagar (22BLC1228), Prabhu Kalyan Panda (22BLC1213)\n**Guide:** Dr. Praveen Jaraut — VIT Bhopal Capstone"}, {"cell_type": "code", "execution_count": null, "id": "c01", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 1: Install deps + Imports + Config\n# ============================================================================\n!pip install pesq==0.0.4 pystoi -q\n\nimport torch, torch.nn as nn, torch.nn.functional as F\nimport torchaudio\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\nimport matplotlib\nmatplotlib.use('Agg')\nimport matplotlib.pyplot as plt\nfrom tqdm.auto import tqdm\nimport glob, os, json, time, warnings, textwrap\nwarnings.filterwarnings('ignore')\nfrom pesq import pesq as pesq_metric\nfrom pystoi import stoi as stoi_metric\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# STFT config\nN_FFT      = 512\nHOP_LENGTH = 256\nN_FREQ     = N_FFT // 2 + 1   # 257\nSR         = 16000\nMAX_LEN    = 48000             # 3 s crop from 16-24 s files\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f'Device: {device}')\nif device == 'cuda':\n    props = torch.cuda.get_device_properties(0)\n    vram = getattr(props, 'total_memory', getattr(props, 'total_mem', 0))\n    print(f'GPU: {torch.cuda.get_device_name(0)} | VRAM: {vram/1e9:.1f}GB')\nprint(f'STFT: n_fft={N_FFT}, hop={HOP_LENGTH}, freq={N_FREQ}')\nprint(f'MAX_LEN={MAX_LEN} ({MAX_LEN/SR:.1f}s crop)')\nprint('Imports OK')"}, {"cell_type": "markdown", "id": "md02", "metadata": {"trusted": true}, "source": "## Dataset: LibriSpeech-Noise\nDownload & extract `earth16/libri-speech-noise-dataset` (7000 train + 105 test WAV pairs).\n\n**Files are 16–24 seconds** at 16 kHz.  We crop 3-second aligned segments for training."}, {"cell_type": "code", "execution_count": null, "id": "c03", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 2: Dataset download & extraction\n# ============================================================================\nimport subprocess, zipfile\n\ndata_base = '/kaggle/working/data'\ndl_tmp    = '/kaggle/working/dl_tmp'\nos.makedirs(data_base, exist_ok=True)\nos.makedirs(dl_tmp, exist_ok=True)\ndone_flag = os.path.join(data_base, '.done')\n\nif os.path.exists(done_flag):\n    print('Dataset already extracted, skipping')\nelse:\n    mounted = '/kaggle/input/libri-speech-noise-dataset'\n    if os.path.isdir(mounted) and len(os.listdir(mounted)) > 0:\n        src = mounted\n        print(f'Using mounted dataset at {src}')\n    else:\n        print('Dataset not mounted, downloading via kaggle API...')\n        subprocess.run(['kaggle', 'datasets', 'download',\n                        'earth16/libri-speech-noise-dataset', '-p', dl_tmp], check=True)\n        zf = os.path.join(dl_tmp, 'libri-speech-noise-dataset.zip')\n        if os.path.exists(zf):\n            with zipfile.ZipFile(zf, 'r') as z:\n                z.extractall(dl_tmp)\n            os.remove(zf)\n        src = dl_tmp\n        print(f'Downloaded to {src}')\n\n    subprocess.run(['apt-get', 'install', '-y', 'p7zip-full'], capture_output=True)\n    for arch in ['train.7z', 'y_train.7z', 'test.7z', 'y_test.7z']:\n        fp = os.path.join(src, arch)\n        if os.path.exists(fp):\n            print(f'Extracting {arch}...')\n            subprocess.run(['7z', 'x', fp, f'-o{data_base}', '-y'], capture_output=True)\n    open(done_flag, 'w').close()\n\ndef find_wav_dir(base, name):\n    for root, dirs, files in os.walk(base):\n        if os.path.basename(root) == name and any(f.endswith('.wav') for f in files):\n            return root\n    return None\n\nnoisy_train = find_wav_dir(data_base, 'train')\nclean_train = find_wav_dir(data_base, 'y_train')\nnoisy_test  = find_wav_dir(data_base, 'test')\nclean_test  = find_wav_dir(data_base, 'y_test')\n\nfor tag, d in [('noisy_train', noisy_train), ('clean_train', clean_train),\n               ('noisy_test', noisy_test), ('clean_test', clean_test)]:\n    n = len(glob.glob(os.path.join(d, '*.wav'))) if d else 0\n    print(f'  {tag}: {d} ({n} files)')\n\n# Sanity: check file durations (use torchaudio.load — .info() removed in newer versions)\nsample_files = sorted(glob.glob(os.path.join(noisy_test, '*.wav')))[:3]\nfor fp in sample_files:\n    wav, sr = torchaudio.load(fp)\n    num_frames = wav.shape[-1]\n    dur = num_frames / sr\n    print(f'  Sample: {os.path.basename(fp)} -> {dur:.2f}s ({num_frames} frames, sr={sr})')"}, {"cell_type": "markdown", "id": "md04", "metadata": {"trusted": true}, "source": "## STFT Dataset (ALIGNED Crops — CRITICAL FIX)\n\n**v1 bug:** `_load_fix()` called independently → noisy from time A, clean from time B.\n\n**v2 fix:** `__getitem__` loads BOTH, picks ONE random start, crops BOTH at same position.\nTest mode uses `start=0` for deterministic evaluation."}, {"cell_type": "code", "execution_count": null, "id": "c05", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 3: STFTSpeechDataset — ALIGNED crops (v2 fix)\n# ============================================================================\nclass STFTSpeechDataset(Dataset):\n    '''STFT dataset with ALIGNED random crops for noisy/clean pairs.\n\n    CRITICAL FIX: v1 generated independent random crops for noisy and clean,\n    meaning the model trained on mismatched audio segments.  Now we use ONE\n    shared crop position for both files.\n    '''\n    def __init__(self, noisy_dir, clean_dir, n_fft=N_FFT, hop_length=HOP_LENGTH,\n                 sr=SR, max_len=MAX_LEN, test_mode=False):\n        self.noisy_files = sorted(glob.glob(os.path.join(noisy_dir, '*.wav')))\n        self.clean_files = sorted(glob.glob(os.path.join(clean_dir, '*.wav')))\n        assert len(self.noisy_files) == len(self.clean_files), \\\n            f'Mismatch: {len(self.noisy_files)} noisy vs {len(self.clean_files)} clean'\n        for nf, cf in zip(self.noisy_files[:3], self.clean_files[:3]):\n            assert os.path.basename(nf) == os.path.basename(cf), \\\n                f'Name mismatch: {os.path.basename(nf)} vs {os.path.basename(cf)}'\n        self.n_fft = n_fft\n        self.hop_length = hop_length\n        self.sr = sr\n        self.max_len = max_len\n        self.test_mode = test_mode\n        self.window = torch.hann_window(n_fft)\n\n    def __len__(self):\n        return len(self.noisy_files)\n\n    def __getitem__(self, idx):\n        noisy_wav, sr_n = torchaudio.load(self.noisy_files[idx])\n        clean_wav, sr_c = torchaudio.load(self.clean_files[idx])\n        noisy_wav = noisy_wav[0]\n        clean_wav = clean_wav[0]\n\n        if sr_n != self.sr:\n            noisy_wav = torchaudio.functional.resample(noisy_wav, sr_n, self.sr)\n        if sr_c != self.sr:\n            clean_wav = torchaudio.functional.resample(clean_wav, sr_c, self.sr)\n\n        min_len = min(noisy_wav.shape[0], clean_wav.shape[0])\n        noisy_wav = noisy_wav[:min_len]\n        clean_wav = clean_wav[:min_len]\n\n        # ── CRITICAL FIX: ONE shared crop for both ──\n        if min_len > self.max_len:\n            if self.test_mode:\n                start = 0\n            else:\n                start = torch.randint(0, min_len - self.max_len, (1,)).item()\n            noisy_wav = noisy_wav[start:start + self.max_len]\n            clean_wav = clean_wav[start:start + self.max_len]\n        elif min_len < self.max_len:\n            pad = self.max_len - min_len\n            noisy_wav = F.pad(noisy_wav, (0, pad))\n            clean_wav = F.pad(clean_wav, (0, pad))\n\n        noisy_stft = torch.stft(noisy_wav, self.n_fft, self.hop_length,\n                                window=self.window, return_complex=True)\n        clean_stft = torch.stft(clean_wav, self.n_fft, self.hop_length,\n                                window=self.window, return_complex=True)\n        return {\n            'noisy_mag':   noisy_stft.abs(),\n            'clean_mag':   clean_stft.abs(),\n            'noisy_phase': torch.angle(noisy_stft),\n            'noisy_wav':   noisy_wav,\n            'clean_wav':   clean_wav,\n        }\n\n# Quick sanity check\n_ds = STFTSpeechDataset(noisy_test, clean_test, test_mode=True)\n_s = _ds[0]\nprint(f'STFTSpeechDataset v2 (aligned crops)')\nprint(f'  noisy_mag:  {_s[\"noisy_mag\"].shape}')\nprint(f'  clean_mag:  {_s[\"clean_mag\"].shape}')\nprint(f'  noisy_wav:  {_s[\"noisy_wav\"].shape}')\n\n_corr = torch.corrcoef(torch.stack([_s['noisy_wav'], _s['clean_wav']]))[0,1].item()\nprint(f'  noisy-clean correlation: {_corr:.4f}  (should be > 0.3 if aligned)')\nassert _corr > 0.1, f'Correlation too low ({_corr:.4f}) — crops may still be misaligned!'\nprint('  ALIGNMENT CHECK PASSED')\ndel _ds, _s"}, {"cell_type": "markdown", "id": "md06", "metadata": {"trusted": true}, "source": "## Model: Dual-Path Transformer (DPT)\n\nSame architecture as v1 — the bug was in data loading, not the model.\n\n**Core innovation:** alternating frequency and time transformer blocks give\nthe model full spectral resolution for frequency-selective noise masking.\n\n```\nCNN Encoder (stride-2 on freq): (B,1,257,T) → (B,128,65,T)\n  → DualPathBlock #1: FreqTransformer(65,128) + TimeTransformer(T,128)\n  → DualPathBlock #2: FreqTransformer(65,128) + TimeTransformer(T,128)\n  → Skip + Interpolate → (B,128,257,T)\n  → CNN Decoder + Sigmoid → mask (B,257,T)\n```"}, {"cell_type": "code", "execution_count": null, "id": "c07", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 4: Dual-Path Transformer Model (same architecture as v1)\n# ============================================================================\nclass DualPathBlock(nn.Module):\n    '''One dual-path block: frequency-transformer + time-transformer.'''\n    def __init__(self, d_model, nhead, dim_ff, dropout):\n        super().__init__()\n        self.freq_transformer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True, norm_first=True)\n        self.time_transformer = nn.TransformerEncoderLayer(\n            d_model, nhead, dim_ff, dropout, batch_first=True, norm_first=True)\n\n    def forward(self, x):\n        B, C, Fr, T = x.shape\n        # Frequency path: attend across freq bins for each time step\n        x_f = x.permute(0, 3, 2, 1).reshape(B * T, Fr, C)\n        x_f = self.freq_transformer(x_f)\n        x = x_f.reshape(B, T, Fr, C).permute(0, 3, 2, 1)\n        # Time path: attend across time for each freq bin\n        x_t = x.permute(0, 2, 3, 1).reshape(B * Fr, T, C)\n        x_t = self.time_transformer(x_t)\n        x = x_t.reshape(B, Fr, T, C).permute(0, 3, 1, 2)\n        return x\n\n\nclass DPTSTFTEnhancer(nn.Module):\n    '''Dual-Path Transformer for STFT-based Speech Enhancement.'''\n    def __init__(self, n_freq=257, d_model=128, nhead=4, num_dp_blocks=2,\n                 dim_ff=512, dropout=0.1):\n        super().__init__()\n        self.n_freq = n_freq\n        self.encoder = nn.Sequential(\n            nn.Conv2d(1, 32, 3, stride=(2, 1), padding=1),\n            nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n            nn.Conv2d(32, d_model, 3, stride=(2, 1), padding=1),\n            nn.BatchNorm2d(d_model), nn.ReLU(inplace=True),\n        )\n        self.dp_blocks = nn.ModuleList([\n            DualPathBlock(d_model, nhead, dim_ff, dropout)\n            for _ in range(num_dp_blocks)\n        ])\n        self.decoder = nn.Sequential(\n            nn.Conv2d(d_model, 64, 3, 1, 1),\n            nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n            nn.Conv2d(64, 1, 1),\n            nn.Sigmoid(),\n        )\n        self._init_weights()\n\n    def _init_weights(self):\n        for m in self.modules():\n            if isinstance(m, (nn.Conv2d, nn.Linear)):\n                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n                if m.bias is not None: nn.init.zeros_(m.bias)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.ones_(m.weight); nn.init.zeros_(m.bias)\n\n    def forward(self, x):\n        B, _, F_orig, T_orig = x.shape\n        h = self.encoder(x)\n        skip = h\n        for block in self.dp_blocks:\n            h = block(h)\n        h = h + skip\n        h = F.interpolate(h, size=(F_orig, T_orig),\n                          mode='bilinear', align_corners=False)\n        return self.decoder(h).squeeze(1)\n\n\nmodel = DPTSTFTEnhancer(n_freq=N_FREQ).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'DPTSTFTEnhancer: {total_params:,} params ({total_params/1e6:.2f}M)')\n\nwith torch.no_grad():\n    dummy = torch.randn(2, 1, N_FREQ, 188).to(device)\n    out = model(dummy)\n    print(f'Input: {dummy.shape} -> Mask: {out.shape}')\n    assert out.shape == (2, N_FREQ, 188)\n    assert out.min().item() >= 0 and out.max().item() <= 1\n    print('Forward pass OK')\n\nenc_p = sum(p.numel() for p in model.encoder.parameters())\ndp_p  = sum(p.numel() for p in model.dp_blocks.parameters())\ndec_p = sum(p.numel() for p in model.decoder.parameters())\nprint(f'  Encoder:     {enc_p:>8,} ({enc_p/total_params*100:.1f}%)')\nprint(f'  DPT blocks:  {dp_p:>8,} ({dp_p/total_params*100:.1f}%)')\nprint(f'  Decoder:     {dec_p:>8,} ({dec_p/total_params*100:.1f}%)')"}, {"cell_type": "code", "execution_count": null, "id": "c08", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 5: SI-SDR utility\n# ============================================================================\ndef si_sdr(estimate, reference):\n    ref = reference - reference.mean()\n    est = estimate  - estimate.mean()\n    dot = torch.sum(ref * est)\n    s_target = dot * ref / (torch.sum(ref**2) + 1e-8)\n    e_noise  = est - s_target\n    return 10 * torch.log10(torch.sum(s_target**2) / (torch.sum(e_noise**2) + 1e-8) + 1e-8)\n\nprint('si_sdr defined')"}, {"cell_type": "markdown", "id": "md09", "metadata": {"trusted": true}, "source": "## Training\nL1 loss on log-magnitude. Adam lr=1e-3 with 3-epoch warmup, then ReduceLROnPlateau.\n**Now with aligned crops** — model sees matched (noisy, clean) pairs for the first time!"}, {"cell_type": "code", "execution_count": null, "id": "c10", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 6: Training setup\n# ============================================================================\nMAX_EPOCHS    = 30\nLR            = 1e-3\nBATCH         = 8\nPATIENCE      = 12\nWARMUP_EPOCHS = 3\nCKPT          = 'dpt_v2_best.pth'\n\nmodel = DPTSTFTEnhancer(n_freq=N_FREQ).to(device)\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f'Params: {total_params:,}')\n\n# Training dataset: test_mode=False (random aligned crops)\nfull_train = STFTSpeechDataset(noisy_train, clean_train, test_mode=False)\nn_val   = int(0.1 * len(full_train))\nn_train = len(full_train) - n_val\ntrain_ds, val_ds = torch.utils.data.random_split(\n    full_train, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n\n# Test dataset: test_mode=True (deterministic crop at start=0)\ntest_ds = STFTSpeechDataset(noisy_test, clean_test, test_mode=True)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n                          num_workers=0, drop_last=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=0)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n    optimizer, 'min', factor=0.5, patience=5)\n\nprint(f'Train:{n_train} Val:{n_val} Test:{len(test_ds)} | BS={BATCH} LR={LR}')\nprint(f'Warmup: {WARMUP_EPOCHS} ep, then ReduceLROnPlateau(patience=5, factor=0.5)')\nprint(f'Train crops: RANDOM aligned | Test crops: DETERMINISTIC (start=0)')"}, {"cell_type": "code", "execution_count": null, "id": "c11", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 7: Training loop with warmup\n# ============================================================================\nhistory = {'train_loss': [], 'val_loss': []}\nbest_val = float('inf')\npatience_ctr = 0\nt0 = time.time()\n\nfor epoch in range(1, MAX_EPOCHS + 1):\n    # LR warmup\n    if epoch <= WARMUP_EPOCHS:\n        warmup_lr = LR * epoch / WARMUP_EPOCHS\n        for pg in optimizer.param_groups:\n            pg['lr'] = warmup_lr\n\n    # --- Train ---\n    model.train()\n    train_losses = []\n    for batch in tqdm(train_loader, desc=f'Ep{epoch}/{MAX_EPOCHS}', leave=False):\n        noisy_mag = batch['noisy_mag'].to(device)\n        clean_mag = batch['clean_mag'].to(device)\n        inp  = torch.log1p(noisy_mag).unsqueeze(1)\n        mask = model(inp)\n        enhanced_mag = mask * noisy_mag\n        loss = F.l1_loss(torch.log1p(enhanced_mag), torch.log1p(clean_mag))\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n        optimizer.step()\n        train_losses.append(loss.item())\n\n    # --- Validate ---\n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            noisy_mag = batch['noisy_mag'].to(device)\n            clean_mag = batch['clean_mag'].to(device)\n            inp  = torch.log1p(noisy_mag).unsqueeze(1)\n            mask = model(inp)\n            enhanced_mag = mask * noisy_mag\n            loss = F.l1_loss(torch.log1p(enhanced_mag), torch.log1p(clean_mag))\n            val_losses.append(loss.item())\n\n    tr_loss = np.mean(train_losses)\n    va_loss = np.mean(val_losses)\n    history['train_loss'].append(tr_loss)\n    history['val_loss'].append(va_loss)\n\n    if epoch > WARMUP_EPOCHS:\n        scheduler.step(va_loss)\n\n    elapsed = time.time() - t0\n    lr_now  = optimizer.param_groups[0]['lr']\n    line = f'Ep{epoch:02d} tr={tr_loss:.4f} va={va_loss:.4f} lr={lr_now:.1e} [{elapsed:.0f}s]'\n\n    if va_loss < best_val:\n        best_val = va_loss\n        patience_ctr = 0\n        torch.save({'epoch': epoch, 'model': model.state_dict(),\n                     'val_loss': float(va_loss)}, CKPT)\n        print(f'{line}  SAVED best={va_loss:.4f}')\n    else:\n        patience_ctr += 1\n        print(f'{line}  no improve ({patience_ctr}/{PATIENCE})')\n\n    if epoch % 5 == 0:\n        torch.save(model.state_dict(), f'ckpt_ep{epoch}.pth')\n\n    if patience_ctr >= PATIENCE:\n        print(f'Early stopping at epoch {epoch}')\n        break\n\nbest_ep = history['val_loss'].index(min(history['val_loss'])) + 1\nprint(f'\\nDONE best_ep={best_ep} best_val={best_val:.4f} time={time.time()-t0:.0f}s')"}, {"cell_type": "markdown", "id": "md12", "metadata": {"trusted": true}, "source": "## Results"}, {"cell_type": "code", "execution_count": null, "id": "c13", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 8: Training curves\n# ============================================================================\nfig, ax = plt.subplots(1, 1, figsize=(10, 5))\neps = range(1, len(history['train_loss']) + 1)\nax.plot(eps, history['train_loss'], 'b-o', label='Train', ms=3)\nax.plot(eps, history['val_loss'], 'r-s', label='Val', ms=3)\nax.axvline(best_ep, color='g', ls='--', alpha=0.7, label=f'Best (ep{best_ep})')\nif WARMUP_EPOCHS > 0:\n    ax.axvline(WARMUP_EPOCHS, color='orange', ls=':', alpha=0.7, label='Warmup end')\nax.set_xlabel('Epoch')\nax.set_ylabel('L1 Loss (log-magnitude)')\nax.set_title('DPT v2 (Aligned Crops) — Training Curves')\nax.legend()\nax.grid(True, alpha=0.3)\nplt.tight_layout()\nplt.savefig('training_curves.png', dpi=150)\nplt.show()\nprint('Saved training_curves.png')"}, {"cell_type": "markdown", "id": "md14", "metadata": {"trusted": true}, "source": "## Evaluation\nPESQ / STOI / SI-SDR on 105 test samples.\n**Deterministic:** test_mode=True → crop at start=0 → same segment every run."}, {"cell_type": "code", "execution_count": null, "id": "c15", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 9: Evaluation — PESQ / STOI / SI-SDR\n# ============================================================================\ntorch.manual_seed(42)\n\nckpt = torch.load(CKPT, map_location=device, weights_only=False)\nmodel.load_state_dict(ckpt['model'])\nmodel.eval()\nprint(f'Loaded: epoch={ckpt[\"epoch\"]}, val_loss={ckpt[\"val_loss\"]:.4f}')\n\nwindow_eval = torch.hann_window(N_FFT).to(device)\n\npesq_noisy_list, pesq_enh_list   = [], []\nstoi_noisy_list, stoi_enh_list   = [], []\nsisdr_noisy_list, sisdr_enh_list = [], []\n\nfor i in tqdm(range(len(test_ds)), desc='Eval'):\n    s           = test_ds[i]\n    noisy_mag   = s['noisy_mag'].unsqueeze(0).to(device)\n    noisy_phase = s['noisy_phase'].unsqueeze(0).to(device)\n    clean_np    = s['clean_wav'].numpy()\n    noisy_np    = s['noisy_wav'].numpy()\n\n    with torch.no_grad():\n        inp     = torch.log1p(noisy_mag).unsqueeze(1)\n        mask    = model(inp)\n        enh_mag = (mask * noisy_mag).squeeze(0)\n\n    enh_stft = enh_mag * torch.exp(1j * noisy_phase.squeeze(0))\n    enh_wav  = torch.istft(enh_stft, N_FFT, HOP_LENGTH,\n                           window=window_eval, length=MAX_LEN)\n    enh_np   = enh_wav.cpu().numpy()\n\n    try:\n        pesq_noisy_list.append(pesq_metric(SR, clean_np, noisy_np, 'wb'))\n        pesq_enh_list.append(  pesq_metric(SR, clean_np, enh_np,   'wb'))\n    except Exception as e:\n        print(f'  PESQ err {i}: {e}')\n\n    stoi_noisy_list.append(stoi_metric(clean_np, noisy_np, SR, extended=False))\n    stoi_enh_list.append(  stoi_metric(clean_np, enh_np,   SR, extended=False))\n\n    c_t = torch.from_numpy(clean_np).float()\n    n_t = torch.from_numpy(noisy_np).float()\n    e_t = torch.from_numpy(enh_np).float()\n    sisdr_noisy_list.append(si_sdr(n_t, c_t).item())\n    sisdr_enh_list.append(  si_sdr(e_t, c_t).item())\n\n    if i < 3:\n        print(f'  [{i}] PESQ: {pesq_noisy_list[-1]:.3f}->{pesq_enh_list[-1]:.3f}  '\n              f'STOI: {stoi_noisy_list[-1]:.3f}->{stoi_enh_list[-1]:.3f}  '\n              f'SI-SDR: {sisdr_noisy_list[-1]:.2f}->{sisdr_enh_list[-1]:.2f}dB')\n\navg = lambda lst: float(np.mean(lst)) if lst else 0.0\navg_pesq_n,  avg_pesq_e  = avg(pesq_noisy_list),  avg(pesq_enh_list)\navg_stoi_n,  avg_stoi_e  = avg(stoi_noisy_list),  avg(stoi_enh_list)\navg_sisdr_n, avg_sisdr_e = avg(sisdr_noisy_list), avg(sisdr_enh_list)\n\nprint(f'\\n{\"=\"*70}')\nprint(f'Results on {len(test_ds)} test files (ALIGNED deterministic crops):')\nprint(f'  PESQ  : noisy={avg_pesq_n:.3f}  enhanced={avg_pesq_e:.3f}  Δ={avg_pesq_e-avg_pesq_n:+.3f}')\nprint(f'  STOI  : noisy={avg_stoi_n:.3f}  enhanced={avg_stoi_e:.3f}  Δ={avg_stoi_e-avg_stoi_n:+.4f}')\nprint(f'  SI-SDR: noisy={avg_sisdr_n:.2f}dB  enh={avg_sisdr_e:.2f}dB  Δ={avg_sisdr_e-avg_sisdr_n:+.2f}dB')\nprint(f'{\"=\"*70}')"}, {"cell_type": "markdown", "id": "md16", "metadata": {"trusted": true}, "source": "## Visualization\nSpectrogram comparison: noisy vs enhanced vs clean, plus predicted mask."}, {"cell_type": "code", "execution_count": null, "id": "c17", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 10: Spectrogram comparison\n# ============================================================================\nsample = test_ds[0]\nnoisy_mag_s = sample['noisy_mag'].unsqueeze(0).to(device)\n\nwith torch.no_grad():\n    inp_s  = torch.log1p(noisy_mag_s).unsqueeze(1)\n    mask_s = model(inp_s)\n    enh_mag_s = (mask_s * noisy_mag_s).squeeze(0).cpu()\n\nnoisy_spec = sample['noisy_mag'].numpy()\nclean_spec = sample['clean_mag'].numpy()\nenh_spec   = enh_mag_s.numpy()\nmask_np    = mask_s.squeeze(0).cpu().numpy()\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\nfor ax, spec, title in [\n    (axes[0,0], np.log1p(noisy_spec), 'Noisy Input'),\n    (axes[0,1], np.log1p(clean_spec), 'Clean Target'),\n    (axes[1,0], np.log1p(enh_spec),   'Enhanced (DPT v2)'),\n    (axes[1,1], mask_np,              'Predicted Mask'),\n]:\n    im = ax.imshow(spec, aspect='auto', origin='lower', cmap='viridis')\n    ax.set_title(title, fontsize=13)\n    ax.set_xlabel('Time frame')\n    ax.set_ylabel('Frequency bin')\n    plt.colorbar(im, ax=ax, fraction=0.046)\n\nplt.suptitle('DPT v2 (Aligned Crops): Spectrogram Comparison', fontsize=14)\nplt.tight_layout()\nplt.savefig('spectrogram_comparison.png', dpi=150)\nplt.show()\nprint('Saved spectrogram_comparison.png')"}, {"cell_type": "code", "execution_count": null, "id": "c18", "metadata": {"trusted": true}, "outputs": [], "source": "# ============================================================================\n# Cell 11: Summary JSON + comparison table\n# ============================================================================\nsummary = {\n    'model': 'DPT_v2_AlignedCrops',\n    'architecture': 'Dual-Path Transformer (Shallow Transformer)',\n    'critical_fix': 'Aligned random crops for noisy/clean pairs',\n    'params': total_params,\n    'checkpoint': {'epoch': int(ckpt['epoch']), 'val_loss': float(ckpt['val_loss'])},\n    'test_samples': len(test_ds),\n    'metrics': {\n        'pesq_noisy':        round(avg_pesq_n, 3),\n        'pesq_enhanced':     round(avg_pesq_e, 3),\n        'stoi_noisy':        round(avg_stoi_n, 4),\n        'stoi_enhanced':     round(avg_stoi_e, 4),\n        'sisdr_noisy_dB':    round(avg_sisdr_n, 2),\n        'sisdr_enhanced_dB': round(avg_sisdr_e, 2),\n    },\n    'history': history,\n}\nwith open('dpt_v2_summary.json', 'w') as f:\n    json.dump(summary, f, indent=2)\nprint('Saved dpt_v2_summary.json')\n\nW = 70\nprint(f'\\n{\"=\"*W}')\nprint(f'{\"Metric\":>10} {\"Noisy\":>12} {\"DPT v2\":>12} {\"Delta\":>12}')\nprint(f'{\"=\"*W}')\nprint(f'{\"PESQ\":>10} {avg_pesq_n:>12.3f} {avg_pesq_e:>12.3f} {avg_pesq_e-avg_pesq_n:>+12.3f}')\nprint(f'{\"STOI\":>10} {avg_stoi_n:>12.3f} {avg_stoi_e:>12.3f} {avg_stoi_e-avg_stoi_n:>+12.4f}')\nprint(f'{\"SI-SDR\":>10} {avg_sisdr_n:>11.2f}dB {avg_sisdr_e:>11.2f}dB {avg_sisdr_e-avg_sisdr_n:>+11.2f}dB')\nprint(f'{\"=\"*W}')\nprint(f'\\nv1 (misaligned) noisy baseline was: STOI=0.218, SI-SDR=-43.34dB (broken)')\nprint(f'v2 (aligned) noisy baseline is: STOI={avg_stoi_n:.3f}, SI-SDR={avg_sisdr_n:.2f}dB (correct)')"}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.10.0"}}, "nbformat": 4, "nbformat_minor": 5}