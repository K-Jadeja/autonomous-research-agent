{
  "hasId": true,
  "id": 110458458,
  "hasText": true,
  "text": "{\"cells\": [{\"cell_type\": \"markdown\", \"id\": \"md00\", \"metadata\": {\"trusted\": true}, \"source\": \"# Review 3 \\u2014 Eval Only: STFT-Transformer Speech Enhancement\\n\\n**Purpose:** Load trained checkpoint from Review 3 training run and compute metrics.  \\n**Skips:** All training. Loads `stft_transformer_best.pth` from kernel output.  \\n**Metrics:** PESQ, STOI, SI-SDR on 105 test pairs + attention visualization.\\n\\n**Team:** Krishnasinh Jadeja (22BLC1211), Kirtan Sondagar (22BLC1228), Prabhu Kalyan Panda (22BLC1213)  \\n**Guide:** Dr. Praveen Jaraut \\u2014 VIT Bhopal Capstone\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c01\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 1: Install deps + Imports + Config\\n# ============================================================================\\n!pip install pesq==0.0.4 pystoi -q\\n\\nimport torch, torch.nn as nn, torch.nn.functional as F\\nimport torchaudio\\nfrom torch.utils.data import Dataset, DataLoader\\nimport numpy as np\\nimport matplotlib\\nmatplotlib.use('Agg')\\nimport matplotlib.pyplot as plt\\nfrom tqdm.auto import tqdm\\nimport glob, os, json, warnings\\nwarnings.filterwarnings('ignore')\\nfrom pesq import pesq as pesq_metric\\nfrom pystoi import stoi as stoi_metric\\n\\ntorch.manual_seed(42)\\nnp.random.seed(42)\\n\\n# STFT config (must match training)\\nN_FFT      = 512\\nHOP_LENGTH = 256\\nN_FREQ     = N_FFT // 2 + 1   # 257\\nSR         = 16000\\nMAX_LEN    = 48000             # 3 s\\n\\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\\nprint(f'Device: {device}')\\nif device == 'cuda':\\n    props = torch.cuda.get_device_properties(0)\\n    vram  = getattr(props, 'total_memory', getattr(props, 'total_mem', 0))\\n    print(f'GPU: {torch.cuda.get_device_name(0)} | VRAM: {vram/1e9:.1f} GB')\\nprint(f'STFT: n_fft={N_FFT}, hop={HOP_LENGTH}, freq_bins={N_FREQ}')\\n\\n# Checkpoint: try mounted input first, fall back to downloading kernel output\\nCKPT_DIR_MOUNTED = '/kaggle/input/review-3-stft-transformer-speech-enhancement'\\nCKPT_DOWNLOAD    = '/kaggle/working/ckpt_dl'\\nCKPT_PATH = os.path.join(CKPT_DIR_MOUNTED, 'stft_transformer_best.pth')\\n\\nif os.path.exists(CKPT_PATH):\\n    print(f'Checkpoint found (mounted): {CKPT_PATH}')\\n    print('Files in input dir:', sorted(os.listdir(CKPT_DIR_MOUNTED)))\\nelse:\\n    print('Mounted checkpoint not found \\u2014 downloading via kaggle kernels output ...')\\n    import subprocess\\n    os.makedirs(CKPT_DOWNLOAD, exist_ok=True)\\n    r = subprocess.run(\\n        ['kaggle', 'kernels', 'output',\\n         'kjadeja/review-3-stft-transformer-speech-enhancement',\\n         '-p', CKPT_DOWNLOAD],\\n        capture_output=True, text=True)\\n    print(r.stdout[-500:] if r.stdout else '(no stdout)')\\n    print(r.stderr[-300:] if r.stderr else '')\\n    # Find the checkpoint in downloaded files\\n    candidates = [os.path.join(CKPT_DOWNLOAD, f)\\n                  for f in os.listdir(CKPT_DOWNLOAD) if 'best' in f and f.endswith('.pth')]\\n    if not candidates:\\n        candidates = [os.path.join(CKPT_DOWNLOAD, f)\\n                      for f in os.listdir(CKPT_DOWNLOAD) if f.endswith('.pth')]\\n    print('Downloaded files:', os.listdir(CKPT_DOWNLOAD))\\n    assert candidates, f'No .pth files found in {CKPT_DOWNLOAD}'\\n    CKPT_PATH = candidates[0]\\n    print(f'Using checkpoint: {CKPT_PATH}')\\n\\nprint(f'CKPT_PATH = {CKPT_PATH}')\\nprint(f'Exists: {os.path.exists(CKPT_PATH)}')\"}, {\"cell_type\": \"markdown\", \"id\": \"md02\", \"metadata\": {\"trusted\": true}, \"source\": \"## Test Data Setup\\nExtracts test/y_test splits from the mounted LibriSpeech-Noise dataset.\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c03\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 2: Extract test data from mounted dataset\\n# ============================================================================\\nimport subprocess, zipfile\\n\\nDATA_BASE  = '/kaggle/working/data'\\nos.makedirs(DATA_BASE, exist_ok=True)\\ndone_flag  = os.path.join(DATA_BASE, '.done_test')\\n\\nif os.path.exists(done_flag):\\n    print('Test data already extracted, skipping')\\nelse:\\n    # Install p7zip\\n    subprocess.run(['apt-get', 'install', '-y', 'p7zip-full'], capture_output=True)\\n\\n    # Try mounted dataset first\\n    ds_mounted = '/kaggle/input/libri-speech-noise-dataset'\\n    dl_tmp     = '/kaggle/working/dl_tmp'\\n    os.makedirs(dl_tmp, exist_ok=True)\\n\\n    if os.path.isdir(ds_mounted) and len(os.listdir(ds_mounted)) > 0:\\n        src = ds_mounted\\n        print(f'Using mounted dataset at {src}')\\n    else:\\n        print('Dataset not mounted \\u2014 downloading via kaggle API ...')\\n        subprocess.run(['kaggle', 'datasets', 'download',\\n                        'earth16/libri-speech-noise-dataset', '-p', dl_tmp], check=True)\\n        zf = os.path.join(dl_tmp, 'libri-speech-noise-dataset.zip')\\n        if os.path.exists(zf):\\n            with zipfile.ZipFile(zf) as z:\\n                z.extractall(dl_tmp)\\n            os.remove(zf)\\n        src = dl_tmp\\n\\n    # Extract ONLY test archives (skip 6 GB train data)\\n    for arch in ['test.7z', 'y_test.7z']:\\n        fp = os.path.join(src, arch)\\n        if os.path.exists(fp):\\n            print(f'Extracting {arch} ...')\\n            r = subprocess.run(['7z', 'x', fp, f'-o{DATA_BASE}', '-y'], capture_output=True)\\n            print(r.stdout.decode()[-300:] if r.stdout else '(no stdout)')\\n        else:\\n            print(f'WARNING: {fp} not found')\\n\\n    open(done_flag, 'w').close()\\n    print('Extraction complete')\\n\\n# Locate directories\\ndef find_wav_dir(base, name):\\n    for root, dirs, files in os.walk(base):\\n        if os.path.basename(root) == name and any(f.endswith('.wav') for f in files):\\n            return root\\n    return None\\n\\nnoisy_test = find_wav_dir(DATA_BASE, 'test')\\nclean_test = find_wav_dir(DATA_BASE, 'y_test')\\nfor tag, d in [('noisy_test', noisy_test), ('clean_test', clean_test)]:\\n    n = len(glob.glob(os.path.join(d, '*.wav'))) if d else 0\\n    print(f'  {tag}: {d} ({n} files)')\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c04\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 3: STFTSpeechDataset\\n# ============================================================================\\nclass STFTSpeechDataset(Dataset):\\n    def __init__(self, noisy_dir, clean_dir, n_fft=N_FFT, hop_length=HOP_LENGTH,\\n                 sr=SR, max_len=MAX_LEN):\\n        self.noisy_files = sorted(glob.glob(os.path.join(noisy_dir, '*.wav')))\\n        self.clean_files = sorted(glob.glob(os.path.join(clean_dir, '*.wav')))\\n        assert len(self.noisy_files) == len(self.clean_files)\\n        self.n_fft = n_fft; self.hop_length = hop_length\\n        self.sr = sr; self.max_len = max_len\\n        self.window = torch.hann_window(n_fft)\\n\\n    def __len__(self): return len(self.noisy_files)\\n\\n    def _load_fix(self, path):\\n        wav, sr = torchaudio.load(path)\\n        if sr != self.sr:\\n            wav = torchaudio.functional.resample(wav, sr, self.sr)\\n        wav = wav[0]\\n        if wav.shape[0] > self.max_len:\\n            start = torch.randint(0, wav.shape[0] - self.max_len, (1,)).item()\\n            wav = wav[start:start + self.max_len]\\n        elif wav.shape[0] < self.max_len:\\n            wav = F.pad(wav, (0, self.max_len - wav.shape[0]))\\n        return wav\\n\\n    def __getitem__(self, idx):\\n        noisy_wav = self._load_fix(self.noisy_files[idx])\\n        clean_wav = self._load_fix(self.clean_files[idx])\\n        noisy_stft = torch.stft(noisy_wav, self.n_fft, self.hop_length,\\n                                window=self.window, return_complex=True)\\n        clean_stft = torch.stft(clean_wav, self.n_fft, self.hop_length,\\n                                window=self.window, return_complex=True)\\n        return {\\n            'noisy_mag':   noisy_stft.abs(),\\n            'clean_mag':   clean_stft.abs(),\\n            'noisy_phase': torch.angle(noisy_stft),\\n            'noisy_wav':   noisy_wav,\\n            'clean_wav':   clean_wav,\\n        }\\n\\ntest_ds = STFTSpeechDataset(noisy_test, clean_test)\\nprint(f'Test samples: {len(test_ds)}')\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c05\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 4: STFTTransformerEnhancer (must match training architecture exactly)\\n# ============================================================================\\nclass ConvBlock(nn.Module):\\n    def __init__(self, in_c, out_c, k=3, s=1, p=1):\\n        super().__init__()\\n        self.net = nn.Sequential(\\n            nn.Conv2d(in_c, out_c, k, s, p),\\n            nn.BatchNorm2d(out_c),\\n            nn.ReLU(inplace=True))\\n    def forward(self, x): return self.net(x)\\n\\nclass PositionalEncoding(nn.Module):\\n    def __init__(self, d_model, dropout=0.1, max_len=2000):\\n        super().__init__()\\n        self.dropout = nn.Dropout(dropout)\\n        pe  = torch.zeros(max_len, d_model)\\n        pos = torch.arange(max_len).unsqueeze(1).float()\\n        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\\n        pe[:, 0::2] = torch.sin(pos * div)\\n        pe[:, 1::2] = torch.cos(pos * div)\\n        self.register_buffer('pe', pe.unsqueeze(0))\\n    def forward(self, x): return self.dropout(x + self.pe[:, :x.size(1)])\\n\\nclass STFTTransformerEnhancer(nn.Module):\\n    def __init__(self, n_freq=257, d_model=256, nhead=4, num_layers=2,\\n                 dim_ff=1024, dropout=0.1):\\n        super().__init__()\\n        self.n_freq = n_freq\\n        self.encoder   = nn.Sequential(ConvBlock(1,64), ConvBlock(64,128), ConvBlock(128,256))\\n        self.pre_proj  = nn.Linear(256, d_model)\\n        self.pos_enc   = PositionalEncoding(d_model, dropout)\\n        enc_layer = nn.TransformerEncoderLayer(\\n            d_model, nhead, dim_ff, dropout, batch_first=True, norm_first=True)\\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\\n        self.post_proj = nn.Linear(d_model, 256)\\n        self.decoder   = nn.Sequential(\\n            ConvBlock(256,128), ConvBlock(128,64),\\n            nn.Conv2d(64,1,3,1,1), nn.Sigmoid())\\n\\n    def forward(self, x):\\n        enc  = self.encoder(x)                                    # (B,256,F,T)\\n        feat = enc.mean(dim=2).permute(0,2,1)                    # (B,T,256)\\n        feat = self.pos_enc(self.pre_proj(feat))\\n        feat = self.post_proj(self.transformer(feat))             # (B,T,256)\\n        feat = feat.permute(0,2,1).unsqueeze(2)                  # (B,256,1,T)\\n        feat = feat.expand(-1,-1,self.n_freq,-1)                 # (B,256,F,T)\\n        return self.decoder(feat).squeeze(1)                     # (B,F,T)\\n\\nmodel = STFTTransformerEnhancer(n_freq=N_FREQ).to(device)\\ntotal_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\\nprint(f'Model: {total_params:,} params ({total_params/1e6:.2f}M)')\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c06\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 5: SI-SDR + attention weight utilities\\n# ============================================================================\\ndef si_sdr(estimate, reference):\\n    ref = reference - reference.mean()\\n    est = estimate  - estimate.mean()\\n    dot = torch.sum(ref * est)\\n    s_target = dot * ref / (torch.sum(ref**2) + 1e-8)\\n    e_noise   = est - s_target\\n    return 10 * torch.log10(torch.sum(s_target**2) / (torch.sum(e_noise**2) + 1e-8) + 1e-8)\\n\\ndef get_attention_weights(mdl, x):\\n    mdl.eval()\\n    weights = []\\n    with torch.no_grad():\\n        enc  = mdl.encoder(x)\\n        feat = enc.mean(dim=2).permute(0,2,1)\\n        feat = mdl.pos_enc(mdl.pre_proj(feat))\\n        for layer in mdl.transformer.layers:\\n            normed = layer.norm1(feat)\\n            _, w   = layer.self_attn(normed, normed, normed,\\n                                     need_weights=True, average_attn_weights=False)\\n            weights.append(w.cpu())\\n            feat = feat + layer.dropout1(\\n                layer.self_attn(normed, normed, normed, need_weights=False)[0])\\n            normed2 = layer.norm2(feat)\\n            feat = feat + layer.linear2(\\n                F.dropout(layer.activation(layer.linear1(normed2)), p=0.0, training=False))\\n    return weights\\n\\nprint('Utilities defined')\"}, {\"cell_type\": \"markdown\", \"id\": \"md07\", \"metadata\": {\"trusted\": true}, \"source\": \"## Evaluation\\nLoad best checkpoint \\u2192 compute PESQ / STOI / SI-SDR on all 105 test pairs.\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c08\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 6: Load checkpoint + full evaluation\\n# ============================================================================\\nassert os.path.exists(CKPT_PATH), f'Checkpoint not found: {CKPT_PATH}'\\nckpt = torch.load(CKPT_PATH, map_location=device, weights_only=False)\\nmodel.load_state_dict(ckpt['model'])\\nmodel.eval()\\nprint(f'Loaded checkpoint: epoch={ckpt[\\\"epoch\\\"]}, val_loss={ckpt[\\\"val_loss\\\"]:.4f}')\\n\\nwindow_gpu = torch.hann_window(N_FFT).to(device)\\n\\npesq_noisy_list, pesq_enh_list  = [], []\\nstoi_noisy_list, stoi_enh_list  = [], []\\nsisdr_noisy_list, sisdr_enh_list = [], []\\n\\nfor i in tqdm(range(len(test_ds)), desc='Eval'):\\n    s           = test_ds[i]\\n    noisy_mag   = s['noisy_mag'].unsqueeze(0).to(device)    # (1,257,T)\\n    noisy_phase = s['noisy_phase'].unsqueeze(0).to(device)  # (1,257,T)\\n    clean_np    = s['clean_wav'].numpy()\\n    noisy_np    = s['noisy_wav'].numpy()\\n\\n    with torch.no_grad():\\n        inp          = torch.log1p(noisy_mag).unsqueeze(1)  # (1,1,257,T)\\n        mask         = model(inp)                            # (1,257,T)\\n        enh_mag      = (mask * noisy_mag).squeeze(0)        # (257,T)\\n\\n    enh_stft = enh_mag * torch.exp(1j * noisy_phase.squeeze(0))\\n    enh_wav  = torch.istft(enh_stft, N_FFT, HOP_LENGTH, window=window_gpu, length=MAX_LEN)\\n    enh_np   = enh_wav.cpu().numpy()\\n\\n    try:\\n        pesq_noisy_list.append(pesq_metric(SR, clean_np, noisy_np, 'wb'))\\n        pesq_enh_list.append(  pesq_metric(SR, clean_np, enh_np,   'wb'))\\n    except Exception as e:\\n        print(f'  PESQ error sample {i}: {e}')\\n\\n    stoi_noisy_list.append(stoi_metric(clean_np, noisy_np, SR, extended=False))\\n    stoi_enh_list.append(  stoi_metric(clean_np, enh_np,   SR, extended=False))\\n\\n    c_t = torch.from_numpy(clean_np).float()\\n    n_t = torch.from_numpy(noisy_np).float()\\n    e_t = torch.from_numpy(enh_np).float()\\n    sisdr_noisy_list.append(si_sdr(n_t, c_t).item())\\n    sisdr_enh_list.append(  si_sdr(e_t, c_t).item())\\n\\navg_pesq_n  = float(np.mean(pesq_noisy_list))  if pesq_noisy_list  else 0.0\\navg_pesq_e  = float(np.mean(pesq_enh_list))    if pesq_enh_list    else 0.0\\navg_stoi_n  = float(np.mean(stoi_noisy_list))\\navg_stoi_e  = float(np.mean(stoi_enh_list))\\navg_sisdr_n = float(np.mean(sisdr_noisy_list))\\navg_sisdr_e = float(np.mean(sisdr_enh_list))\\n\\nprint(f'\\\\nResults on {len(test_ds)} test files:')\\nprint(f'  PESQ  : noisy={avg_pesq_n:.3f}  enhanced={avg_pesq_e:.3f}  \\u0394={avg_pesq_e-avg_pesq_n:+.3f}')\\nprint(f'  STOI  : noisy={avg_stoi_n:.3f}  enhanced={avg_stoi_e:.3f}  \\u0394={avg_stoi_e-avg_stoi_n:+.4f}')\\nprint(f'  SI-SDR: noisy={avg_sisdr_n:.2f}dB  enhanced={avg_sisdr_e:.2f}dB  \\u0394={avg_sisdr_e-avg_sisdr_n:+.2f}dB')\\nprint(f'\\\\n--- Review comparison ---')\\nprint(f'  R1 CRN       (estimated) : PESQ ~ 3.10')\\nprint(f'  R2 Trans+Mel (GriffinLim): PESQ = 1.141  SI-SDR = -25.58 dB')\\nprint(f'  R3 Trans+STFT (ISTFT)    : PESQ = {avg_pesq_e:.3f}  SI-SDR = {avg_sisdr_e:.2f} dB')\"}, {\"cell_type\": \"markdown\", \"id\": \"md09\", \"metadata\": {\"trusted\": true}, \"source\": \"## Attention Visualization\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c10\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 7: Attention heatmaps (2 layers \\u00d7 4 heads)\\n# ============================================================================\\nsample = test_ds[0]\\ninp_t  = torch.log1p(sample['noisy_mag'].unsqueeze(0).unsqueeze(0)).to(device)\\nattn   = get_attention_weights(model, inp_t)\\n\\nfig, axes = plt.subplots(2, 4, figsize=(20, 8))\\nfor li, aw in enumerate(attn):\\n    for hi in range(4):\\n        ax = axes[li, hi]\\n        w  = aw[0, hi].numpy()\\n        ax.imshow(w[:64, :64], aspect='auto', cmap='viridis')\\n        ax.set_title(f'Layer {li+1}  Head {hi+1}', fontsize=11)\\n        ax.set_xlabel('Key frame')\\n        ax.set_ylabel('Query frame')\\nplt.suptitle('Self-Attention Weights \\u2014 first 64 frames (test sample 0)', fontsize=14)\\nplt.tight_layout()\\nplt.savefig('attention_weights.png', dpi=150)\\nplt.show()\\nprint('Saved attention_weights.png')\"}, {\"cell_type\": \"code\", \"execution_count\": null, \"id\": \"c11\", \"metadata\": {\"trusted\": true}, \"outputs\": [], \"source\": \"# ============================================================================\\n# Cell 8: Save review3_summary.json + pretty table\\n# ============================================================================\\nsummary = {\\n    'review': 3,\\n    'run': 'eval_only',\\n    'model': 'STFTTransformerEnhancer',\\n    'pipeline': 'STFT (n_fft=512, hop=256) \\u2192 mask \\u2192 ISTFT',\\n    'params': total_params,\\n    'checkpoint': {'epoch': int(ckpt['epoch']), 'val_loss': float(ckpt['val_loss'])},\\n    'test_samples': len(test_ds),\\n    'metrics': {\\n        'pesq_noisy':    round(avg_pesq_n, 3),\\n        'pesq_enhanced': round(avg_pesq_e, 3),\\n        'stoi_noisy':    round(avg_stoi_n, 4),\\n        'stoi_enhanced': round(avg_stoi_e, 4),\\n        'sisdr_noisy_dB':    round(avg_sisdr_n, 2),\\n        'sisdr_enhanced_dB': round(avg_sisdr_e, 2),\\n    },\\n    'comparison': {\\n        'R1_CRN_PESQ_estimated': 3.10,\\n        'R2_TransMel_PESQ': 1.141,\\n        'R2_TransMel_SISDR_dB': -25.58,\\n        'R3_TransSTFT_PESQ': round(avg_pesq_e, 3),\\n        'R3_TransSTFT_SISDR_dB': round(avg_sisdr_e, 2),\\n    }\\n}\\nwith open('review3_summary.json', 'w') as f:\\n    json.dump(summary, f, indent=2)\\nprint('Saved review3_summary.json')\\n\\nW = 72\\nprint('\\\\n' + '='*W)\\nprint(f'{\\\"Review\\\":>10} {\\\"Pipeline\\\":>22} {\\\"PESQ\\\":>8} {\\\"STOI\\\":>7} {\\\"SI-SDR\\\":>10} {\\\"Params\\\":>8}')\\nprint('='*W)\\nprint(f'{\\\"R1 CRN\\\":>10} {\\\"Mel (estimated)\\\":>22} {\\\"~3.10\\\":>8} {\\\"\\u2014\\\":>7} {\\\"\\u2014\\\":>10} {\\\"~2.5M\\\":>8}')\\nprint(f'{\\\"R2 Trans\\\":>10} {\\\"Mel+GriffinLim\\\":>22} {1.141:>8.3f} {0.695:>7.3f} {-25.58:>9.2f}dB {\\\"2.45M\\\":>8}')\\nprint(f'{\\\"R3 Trans\\\":>10} {\\\"STFT+ISTFT\\\":>22} {avg_pesq_e:>8.3f} {avg_stoi_e:>7.3f} {avg_sisdr_e:>9.2f}dB {total_params/1e6:>7.2f}M')\\nprint('='*W)\\nprint(f'Noisy baseline: PESQ={avg_pesq_n:.3f}  STOI={avg_stoi_n:.3f}  SI-SDR={avg_sisdr_n:.2f}dB')\\nprint(f'R3 improvement over noisy: dPESQ={avg_pesq_e-avg_pesq_n:+.3f}  dSTOI={avg_stoi_e-avg_stoi_n:+.4f}  dSI-SDR={avg_sisdr_e-avg_sisdr_n:+.2f}dB')\"}], \"metadata\": {\"kernelspec\": {\"display_name\": \"Python 3\", \"language\": \"python\", \"name\": \"python3\"}, \"language_info\": {\"name\": \"python\", \"version\": \"3.10.0\"}}, \"nbformat\": 4, \"nbformat_minor\": 5}",
  "hasNewTitle": true,
  "newTitle": "Review 3 Eval STFT Transformer",
  "hasLanguage": true,
  "language": "python",
  "hasKernelType": true,
  "kernelType": "notebook",
  "hasKernelExecutionType": true,
  "kernelExecutionType": "SaveAndRunAll",
  "hasEnableGpu": true,
  "enableGpu": true,
  "hasEnableInternet": true,
  "enableInternet": true,
  "kernelDataSources": [
    "kjadeja/review-3-stft-transformer-speech-enhancement"
  ],
  "datasetDataSources": [
    "earth16/libri-speech-noise-dataset"
  ]
}