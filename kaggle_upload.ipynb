{"cells": [{"cell_type": "markdown", "id": "c948fc3b", "metadata": {}, "source": ["# Review 2: Lightweight Speech Enhancement \u2014 CNN + Shallow Transformer\n", "**Project:** Capstone \u2014 Lightweight Speech Enhancement Using Shallow Transformers  \n", "**Authors:** Krishnasinh Jadeja (22BLC1211), Kirtan Sondagar (22BLC1228), Prabhu Kalyan Panda (22BLC1213)  \n", "**Phase:** Review 2 \u2014 Replace CRN LSTM with 2-Layer Shallow Transformer  \n", "**Target:** PESQ \u2265 3.2 | ~350K params | < 15ms latency"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "5c8de8c0", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 1: Install dependencies & imports\n", "# ============================================================\n", "!pip install pesq==0.0.4 pystoi -q\n", "\n", "import torch\n", "import torch.nn as nn\n", "import torch.optim as optim\n", "from torch.utils.data import Dataset, DataLoader\n", "import torchaudio\n", "import torchaudio.transforms as T\n", "\n", "import numpy as np\n", "import pandas as pd\n", "import matplotlib.pyplot as plt\n", "from tqdm.auto import tqdm\n", "import glob\n", "import os\n", "import math\n", "import warnings\n", "warnings.filterwarnings('ignore')\n", "\n", "from pesq import pesq\n", "from pystoi import stoi\n", "\n", "torch.manual_seed(42)\n", "np.random.seed(42)\n", "\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "print(f\"\\n Device: {device}\")\n", "if device == 'cuda':\n", "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n", "    print(f\"  VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n", "\n", "print(\"\\n All imports successful!\")"]}, {"cell_type": "markdown", "id": "d9ca688a", "metadata": {}, "source": ["## Dataset Loading & Extraction"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "3bbbe047", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 2: Extract dataset from .7z archives\n", "# ============================================================\n", "import os, glob\n", "\n", "print(\" Installing 7zip...\")\n", "!apt-get install -y p7zip-full > /dev/null 2>&1\n", "print(\" 7zip ready\")\n", "\n", "dataset_path = '/kaggle/input/libri-speech-noise-dataset'\n", "extract_path = '/kaggle/working/extracted_data'\n", "os.makedirs(extract_path, exist_ok=True)\n", "\n", "archives = ['train.7z', 'test.7z', 'y_train.7z', 'y_test.7z']\n", "\n", "already_extracted = all(\n", "    os.path.exists(f'{extract_path}/{f}') and len(glob.glob(f'{extract_path}/{f}/*.wav')) > 0\n", "    for f in ['train', 'y_train', 'test', 'y_test']\n", ")\n", "\n", "if already_extracted:\n", "    print(\" Data already extracted, skipping...\")\n", "else:\n", "    print(\"\\n Extracting archives (5-10 min)...\")\n", "    for archive in archives:\n", "        archive_path = os.path.join(dataset_path, archive)\n", "        if os.path.exists(archive_path):\n", "            print(f\"  -> {archive}...\")\n", "            !7z x \"{archive_path}\" -o\"{extract_path}\" -y > /dev/null 2>&1\n", "            print(f\"     done\")\n", "        else:\n", "            print(f\"     NOT FOUND: {archive}\")\n", "    print(\"\\n Extraction complete!\")\n", "\n", "print(\"\\n Dataset structure:\")\n", "for folder in ['train', 'y_train', 'test', 'y_test']:\n", "    path = f'{extract_path}/{folder}'\n", "    if os.path.exists(path):\n", "        count = len(glob.glob(f'{path}/*.wav'))\n", "        print(f\"  {folder:10s}: {count:,} files\")\n", "    else:\n", "        print(f\"  {folder:10s}: NOT FOUND\")"]}, {"cell_type": "markdown", "id": "be423afd", "metadata": {}, "source": ["## Dataset Class"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "c66b3470", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 3: SpeechEnhancementDataset\n", "# ============================================================\n", "class SpeechEnhancementDataset(Dataset):\n", "    def __init__(self, data_path, split='train', max_samples=None,\n", "                 sr=16000, n_fft=512, hop_length=256, n_mels=128):\n", "        self.sr = sr\n", "        self.n_fft = n_fft\n", "        self.hop_length = hop_length\n", "        self.n_mels = n_mels\n", "        self.segment_length = int(3.0 * sr)\n", "\n", "        print(f\"\\n Loading {split} split...\")\n", "\n", "        if split in ['train', 'val']:\n", "            noisy_dir = f'{data_path}/train'\n", "            clean_dir = f'{data_path}/y_train'\n", "        else:\n", "            noisy_dir = f'{data_path}/test'\n", "            clean_dir = f'{data_path}/y_test'\n", "\n", "        noisy_files = sorted(glob.glob(f'{noisy_dir}/*.wav'))\n", "        clean_files = sorted(glob.glob(f'{clean_dir}/*.wav'))\n", "        print(f\"   Noisy: {len(noisy_files)} | Clean: {len(clean_files)}\")\n", "\n", "        noisy_dict = {os.path.basename(f): f for f in noisy_files}\n", "        clean_dict = {os.path.basename(f): f for f in clean_files}\n", "\n", "        self.pairs = []\n", "        for basename in noisy_dict:\n", "            if basename in clean_dict:\n", "                self.pairs.append((clean_dict[basename], noisy_dict[basename]))\n", "        print(f\"   Matched pairs: {len(self.pairs)}\")\n", "\n", "        if split == 'train':\n", "            idx = int(0.9 * len(self.pairs))\n", "            self.pairs = self.pairs[:idx]\n", "        elif split == 'val':\n", "            idx = int(0.9 * len(self.pairs))\n", "            self.pairs = self.pairs[idx:]\n", "\n", "        if max_samples:\n", "            self.pairs = self.pairs[:max_samples]\n", "\n", "        self.n_samples = len(self.pairs)\n", "        print(f\" {split}: {self.n_samples} samples\")\n", "\n", "        self.mel_spec = T.MelSpectrogram(\n", "            sample_rate=sr, n_fft=n_fft, hop_length=hop_length,\n", "            n_mels=n_mels, power=2.0\n", "        )\n", "\n", "    def __len__(self):\n", "        return self.n_samples\n", "\n", "    def __getitem__(self, idx):\n", "        try:\n", "            clean_path, noisy_path = self.pairs[idx]\n", "            clean_wav, sr = torchaudio.load(clean_path)\n", "            noisy_wav, _  = torchaudio.load(noisy_path)\n", "\n", "            if clean_wav.shape[0] > 1: clean_wav = clean_wav.mean(dim=0, keepdim=True)\n", "            if noisy_wav.shape[0] > 1: noisy_wav = noisy_wav.mean(dim=0, keepdim=True)\n", "\n", "            if sr != self.sr:\n", "                resampler = T.Resample(sr, self.sr)\n", "                clean_wav = resampler(clean_wav)\n", "                noisy_wav = resampler(noisy_wav)\n", "\n", "            if clean_wav.shape[1] < self.segment_length:\n", "                pad = self.segment_length - clean_wav.shape[1]\n", "                clean_wav = torch.nn.functional.pad(clean_wav, (0, pad))\n", "                noisy_wav = torch.nn.functional.pad(noisy_wav, (0, pad))\n", "            else:\n", "                max_start = clean_wav.shape[1] - self.segment_length\n", "                start = np.random.randint(0, max(1, max_start))\n", "                clean_wav = clean_wav[:, start:start + self.segment_length]\n", "                noisy_wav = noisy_wav[:, start:start + self.segment_length]\n", "\n", "            clean_spec = torch.log1p(self.mel_spec(clean_wav)).squeeze(0)\n", "            noisy_spec = torch.log1p(self.mel_spec(noisy_wav)).squeeze(0)\n", "\n", "            return {\n", "                'noisy_spec': noisy_spec,\n", "                'clean_spec': clean_spec,\n", "                'noisy_wav': noisy_wav.squeeze(0),\n", "                'clean_wav': clean_wav.squeeze(0)\n", "            }\n", "        except Exception as e:\n", "            print(f\" Error at {idx}: {e}\")\n", "            return self.__getitem__((idx + 1) % self.n_samples)\n", "\n", "print(\" SpeechEnhancementDataset class defined OK\")"]}, {"cell_type": "markdown", "id": "b75baa99", "metadata": {}, "source": ["## CNN-Transformer Architecture (Review 2)"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "1b233d62", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 4: Positional Encoding\n", "# ============================================================\n", "class PositionalEncoding(nn.Module):\n", "    \"\"\"Sinusoidal positional encoding for Transformer.\"\"\"\n", "    def __init__(self, d_model: int, max_len: int = 512, dropout: float = 0.1):\n", "        super().__init__()\n", "        self.dropout = nn.Dropout(p=dropout)\n", "        pe = torch.zeros(max_len, d_model)\n", "        position = torch.arange(0, max_len).unsqueeze(1).float()\n", "        div_term = torch.exp(\n", "            torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)\n", "        )\n", "        pe[:, 0::2] = torch.sin(position * div_term)\n", "        pe[:, 1::2] = torch.cos(position * div_term)\n", "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, max_len, d_model)\n", "\n", "    def forward(self, x):\n", "        x = x + self.pe[:, :x.size(1), :]\n", "        return self.dropout(x)\n", "\n", "# Quick test\n", "_pe = PositionalEncoding(d_model=256)\n", "_x  = torch.randn(2, 188, 256)\n", "_out = _pe(_x)\n", "print(f\" PositionalEncoding: {_x.shape} -> {_out.shape}  OK\")"]}, {"cell_type": "code", "execution_count": null, "id": "4dd3f89c", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 5: ShallowTransformerEnhancer model\n", "# ============================================================\n", "class ConvBlock(nn.Module):\n", "    def __init__(self, in_ch, out_ch, kernel_size=3, stride=1, padding=1):\n", "        super().__init__()\n", "        self.net = nn.Sequential(\n", "            nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n", "            nn.BatchNorm2d(out_ch),\n", "            nn.ReLU(inplace=True)\n", "        )\n", "    def forward(self, x):\n", "        return self.net(x)\n", "\n", "\n", "class ShallowTransformerEnhancer(nn.Module):\n", "    \"\"\"\n", "    CNN-Transformer hybrid for speech enhancement.\n", "    CNN Encoder:    1 -> 64 -> 128 -> 256  (3x3 Conv, BN, ReLU)\n", "    Transformer:    2 layers, 4 heads, d=256, ff=1024, Pre-LN\n", "    CNN Decoder:    256 -> 128 -> 64 -> 1 + Sigmoid mask\n", "    Output:         enhanced = mask * noisy_spec\n", "    \"\"\"\n", "    def __init__(self, n_mels=128, d_model=256, nhead=4,\n", "                 num_layers=2, ff_dim=1024, dropout=0.1):\n", "        super().__init__()\n", "        self.n_mels  = n_mels\n", "        self.d_model = d_model\n", "\n", "        # CNN Encoder\n", "        self.encoder = nn.Sequential(\n", "            ConvBlock(1,       64),\n", "            ConvBlock(64,      128),\n", "            ConvBlock(128,     d_model),\n", "        )\n", "\n", "        # Pre-Transformer linear projection\n", "        self.pre_proj = nn.Linear(d_model, d_model)\n", "\n", "        # Positional encoding\n", "        self.pos_enc  = PositionalEncoding(d_model=d_model, dropout=dropout)\n", "\n", "        # Shallow Transformer (2 layers, Pre-LN for stability)\n", "        encoder_layer = nn.TransformerEncoderLayer(\n", "            d_model=d_model,\n", "            nhead=nhead,\n", "            dim_feedforward=ff_dim,\n", "            dropout=dropout,\n", "            batch_first=True,\n", "            norm_first=True\n", "        )\n", "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n", "\n", "        # Post-Transformer projection\n", "        self.post_proj = nn.Linear(d_model, d_model)\n", "\n", "        # CNN Decoder\n", "        self.decoder = nn.Sequential(\n", "            ConvBlock(d_model, 128),\n", "            ConvBlock(128,     64),\n", "            nn.Conv2d(64, 1, kernel_size=1)\n", "        )\n", "        self.sigmoid = nn.Sigmoid()\n", "\n", "    def forward(self, noisy_spec):\n", "        \"\"\"\n", "        noisy_spec: (B, n_mels, T)\n", "        Returns:    enhanced (B, n_mels, T), mask (B, n_mels, T)\n", "        \"\"\"\n", "        B, n_mels, T = noisy_spec.shape\n", "\n", "        # CNN encode\n", "        x = self.encoder(noisy_spec.unsqueeze(1))  # (B, 256, 128, T)\n", "\n", "        # Frequency-mean pooling -> temporal sequence\n", "        x = x.permute(0, 3, 2, 1).mean(dim=2)     # (B, T, 256)\n", "        x = self.pre_proj(x)\n", "\n", "        # Positional encoding + Transformer\n", "        x = self.pos_enc(x)\n", "        x = self.transformer(x)                    # (B, T, 256)\n", "\n", "        x = self.post_proj(x)\n", "\n", "        # Reshape back to 2D feature map\n", "        x = x.unsqueeze(2).expand(-1, -1, n_mels, -1)  # (B, T, 128, 256)\n", "        x = x.permute(0, 3, 2, 1)                       # (B, 256, 128, T)\n", "\n", "        # CNN decode -> mask\n", "        mask     = self.sigmoid(self.decoder(x)).squeeze(1)  # (B, 128, T)\n", "        enhanced = mask * noisy_spec\n", "        return enhanced, mask\n", "\n", "\n", "# Build model\n", "print(\" Building ShallowTransformerEnhancer...\")\n", "model = ShallowTransformerEnhancer(\n", "    n_mels=128, d_model=256, nhead=4, num_layers=2, ff_dim=1024, dropout=0.1\n", ").to(device)\n", "\n", "total_params     = sum(p.numel() for p in model.parameters())\n", "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n", "\n", "print(f\"\\n Model Architecture:\")\n", "print(f\"   d_model    : 256\")\n", "print(f\"   nhead      : 4\")\n", "print(f\"   num_layers : 2 (shallow!)\")\n", "print(f\"   ff_dim     : 1024\")\n", "print(f\"   Total params     : {total_params:,}\")\n", "print(f\"   Trainable params : {trainable_params:,}\")\n", "\n", "if total_params < 500_000:\n", "    print(f\" LIGHTWEIGHT: {total_params/1e3:.0f}K params\")\n", "elif total_params < 2_000_000:\n", "    print(f\" OK: {total_params/1e6:.2f}M params\")\n", "else:\n", "    print(f\" WARNING: {total_params/1e6:.2f}M params \u2014 may be heavy\")\n", "\n", "# Test forward pass\n", "dummy = torch.randn(2, 128, 188).to(device)\n", "with torch.no_grad():\n", "    enhanced, mask = model(dummy)\n", "\n", "print(f\"\\n Forward pass: {dummy.shape} -> enhanced {enhanced.shape}, mask {mask.shape}\")\n", "assert enhanced.shape == dummy.shape, f\"Shape mismatch: {enhanced.shape}\"\n", "assert mask.shape == dummy.shape\n", "print(\" Forward pass PASSED\")"]}, {"cell_type": "code", "execution_count": null, "id": "579618b3", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 6: Attention weight extraction helper\n", "# ============================================================\n", "def get_attention_weights(model, noisy_spec_batch):\n", "    \"\"\"\n", "    Extract per-layer, per-head attention maps.\n", "    Returns list of (B, nhead, T, T) tensors.\n", "    \"\"\"\n", "    model.eval()\n", "    attention_maps = []\n", "\n", "    B, n_mels, T = noisy_spec_batch.shape\n", "    x = model.encoder(noisy_spec_batch.unsqueeze(1))\n", "    x = x.permute(0, 3, 2, 1).mean(dim=2)\n", "    x = model.pre_proj(x)\n", "    x = model.pos_enc(x)\n", "\n", "    for layer in model.transformer.layers:\n", "        # Extract attn weights manually (bypass need_weights default)\n", "        attn_out, attn_weights = layer.self_attn(\n", "            x, x, x, need_weights=True, average_attn_weights=False\n", "        )  # attn_weights: (B, nhead, T, T)\n", "        attention_maps.append(attn_weights.detach().cpu())\n", "        x = layer(x)  # continue through the layer\n", "\n", "    return attention_maps\n", "\n", "\n", "# Test\n", "print(\" Testing attention extraction...\")\n", "_dummy = torch.randn(1, 128, 188).to(device)\n", "with torch.no_grad():\n", "    _attn_maps = get_attention_weights(model, _dummy)\n", "\n", "for i, attn in enumerate(_attn_maps):\n", "    print(f\"   Layer {i+1}: {attn.shape}  (B, nhead, T, T)\")\n", "print(\" Attention extraction PASSED\")"]}, {"cell_type": "markdown", "id": "0c083b36", "metadata": {}, "source": ["## Training Setup"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "03d55020", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 7: Training configuration & dataset loading\n", "# ============================================================\n", "LEARNING_RATE = 1e-3\n", "NUM_EPOCHS    = 25\n", "BATCH_SIZE    = 16\n", "PATIENCE      = 10\n", "CHECKPOINT    = 'transformer_best.pth'\n", "\n", "def init_weights(m):\n", "    if isinstance(m, (nn.Conv2d, nn.Linear)):\n", "        nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n", "        if m.bias is not None:\n", "            nn.init.zeros_(m.bias)\n", "    elif isinstance(m, nn.BatchNorm2d):\n", "        nn.init.ones_(m.weight)\n", "        nn.init.zeros_(m.bias)\n", "\n", "start_epoch   = 0\n", "best_val_loss = float('inf')\n", "patience_ctr  = 0\n", "history       = {'train_loss': [], 'val_loss': [], 'best_epoch': 0}\n", "\n", "if os.path.exists(CHECKPOINT):\n", "    print(f\" Checkpoint found: {CHECKPOINT} \u2014 resuming...\")\n", "    ckpt = torch.load(CHECKPOINT)\n", "    model.load_state_dict(ckpt['model_state_dict'])\n", "    start_epoch   = ckpt['epoch']\n", "    best_val_loss = ckpt['val_loss']\n", "    if 'history' in ckpt:\n", "        history = ckpt['history']\n", "    print(f\"   Epoch {start_epoch}, best val_loss={best_val_loss:.4f}\")\n", "else:\n", "    print(\" No checkpoint \u2014 fresh training\")\n", "    model.apply(init_weights)\n", "    print(\" Weights initialised (Kaiming)\")\n", "\n", "print(\"\\n Loading datasets...\")\n", "train_ds = SpeechEnhancementDataset(extract_path, split='train')\n", "val_ds   = SpeechEnhancementDataset(extract_path, split='val')\n", "test_ds  = SpeechEnhancementDataset(extract_path, split='test')\n", "\n", "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=0, pin_memory=True)\n", "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=0, pin_memory=True)\n", "\n", "print(f\"\\n Train : {len(train_ds):,} samples  ({len(train_loader)} batches)\")\n", "print(f\"  Val  : {len(val_ds):,} samples  ({len(val_loader)} batches)\")\n", "print(f\"  Test : {len(test_ds):,} samples\")\n", "\n", "criterion = nn.L1Loss()\n", "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n", "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n", "\n", "if os.path.exists(CHECKPOINT) and 'optimizer_state_dict' in ckpt:\n", "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n", "    print(\" Optimizer state restored\")\n", "\n", "print(f\"\\n Config:\")\n", "print(f\"   Params  : {sum(p.numel() for p in model.parameters()):,}\")\n", "print(f\"   Batch   : {BATCH_SIZE}\")\n", "print(f\"   LR      : {LEARNING_RATE}\")\n", "print(f\"   Epochs  : {NUM_EPOCHS}  (start epoch {start_epoch+1})\")\n", "print(f\"   Patience: {PATIENCE}\")"]}, {"cell_type": "markdown", "id": "faf04c3e", "metadata": {}, "source": ["## Training Loop"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "a583e4f3", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 8: Training loop\n", "# ============================================================\n", "for epoch in range(start_epoch, NUM_EPOCHS):\n", "\n", "    # TRAIN\n", "    model.train()\n", "    train_losses = []\n", "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [TRAIN]\")\n", "    for batch in pbar:\n", "        noisy   = batch['noisy_spec'].to(device)\n", "        clean   = batch['clean_spec'].to(device)\n", "        optimizer.zero_grad()\n", "        enhanced, _ = model(noisy)\n", "        loss = criterion(enhanced, clean)\n", "        loss.backward()\n", "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n", "        optimizer.step()\n", "        train_losses.append(loss.item())\n", "        pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n", "\n", "    avg_train = np.mean(train_losses)\n", "    history['train_loss'].append(avg_train)\n", "\n", "    # VALIDATE\n", "    model.eval()\n", "    val_losses = []\n", "    with torch.no_grad():\n", "        for batch in tqdm(val_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS} [VAL]\"):\n", "            noisy   = batch['noisy_spec'].to(device)\n", "            clean   = batch['clean_spec'].to(device)\n", "            enhanced, _ = model(noisy)\n", "            val_losses.append(criterion(enhanced, clean).item())\n", "\n", "    avg_val = np.mean(val_losses)\n", "    history['val_loss'].append(avg_val)\n", "    scheduler.step(avg_val)\n", "\n", "    print(f\"\\n{'='*65}\")\n", "    print(f\" Epoch {epoch+1}/{NUM_EPOCHS}\")\n", "    print(f\"   Train Loss : {avg_train:.4f}\")\n", "    print(f\"   Val Loss   : {avg_val:.4f}\")\n", "    print(f\"   LR         : {optimizer.param_groups[0]['lr']:.6f}\")\n", "\n", "    if avg_val < best_val_loss:\n", "        best_val_loss = avg_val\n", "        patience_ctr  = 0\n", "        history['best_epoch'] = epoch + 1\n", "        torch.save({\n", "            'epoch':                epoch + 1,\n", "            'model_state_dict':     model.state_dict(),\n", "            'optimizer_state_dict': optimizer.state_dict(),\n", "            'scheduler_state_dict': scheduler.state_dict(),\n", "            'val_loss':             avg_val,\n", "            'train_loss':           avg_train,\n", "            'history':              history,\n", "        }, CHECKPOINT)\n", "        print(f\"    Saved best model (val_loss={avg_val:.4f})\")\n", "    else:\n", "        patience_ctr += 1\n", "        print(f\"   No improvement ({patience_ctr}/{PATIENCE})\")\n", "\n", "    print(f\"{'='*65}\")\n", "\n", "    if patience_ctr >= PATIENCE:\n", "        print(f\"\\n Early stopping at epoch {epoch+1}\")\n", "        break\n", "\n", "    if (epoch + 1) % 5 == 0:\n", "        torch.save({'epoch': epoch+1, 'model_state_dict': model.state_dict(), 'history': history},\n", "                   f'transformer_epoch{epoch+1}.pth')\n", "        print(f\" Checkpoint: transformer_epoch{epoch+1}.pth\")\n", "\n", "print(\"\\n\" + \"=\"*65)\n", "print(\" TRAINING COMPLETE\")\n", "print(f\"  Best epoch    : {history['best_epoch']}\")\n", "print(f\"  Best val loss : {best_val_loss:.4f}\")\n", "print(\"=\"*65)"]}, {"cell_type": "markdown", "id": "8d8a7d17", "metadata": {}, "source": ["## Training Curves"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "bf7f267b", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 9: Plot training curves\n", "# ============================================================\n", "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n", "ep = range(1, len(history['train_loss']) + 1)\n", "\n", "axes[0].plot(ep, history['train_loss'], 'b-o', label='Train', lw=2, ms=4)\n", "axes[0].plot(ep, history['val_loss'],   'r-s', label='Val',   lw=2, ms=4)\n", "axes[0].axvline(history['best_epoch'], color='green', ls='--', lw=2,\n", "                label=f\"Best epoch {history['best_epoch']}\")\n", "axes[0].set_xlabel('Epoch'); axes[0].set_ylabel('L1 Loss (log-mel)')\n", "axes[0].set_title('CNN-Transformer: Train & Val Loss', fontweight='bold')\n", "axes[0].legend(); axes[0].grid(alpha=0.3)\n", "\n", "diff = [t - v for t, v in zip(history['train_loss'], history['val_loss'])]\n", "axes[1].plot(ep, diff, 'purple', lw=2)\n", "axes[1].axhline(0, color='black', lw=1)\n", "axes[1].fill_between(ep, diff, 0,\n", "    where=[d > 0 for d in diff], color='orange', alpha=0.3, label='Overfit')\n", "axes[1].fill_between(ep, diff, 0,\n", "    where=[d <= 0 for d in diff], color='green', alpha=0.3, label='Underfit')\n", "axes[1].set_xlabel('Epoch'); axes[1].set_ylabel('Train - Val Loss')\n", "axes[1].set_title('Generalisation Gap', fontweight='bold')\n", "axes[1].legend(); axes[1].grid(alpha=0.3)\n", "\n", "plt.tight_layout()\n", "plt.savefig('transformer_training_curves.png', dpi=150, bbox_inches='tight')\n", "plt.show()\n", "print(f\" Best epoch: {history['best_epoch']}  |  Best val loss: {best_val_loss:.4f}\")"]}, {"cell_type": "markdown", "id": "e36f57d0", "metadata": {}, "source": ["## Evaluation \u2014 PESQ / STOI / SI-SDR (Griffin-Lim waveform reconstruction)"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "1a464098", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 10: Griffin-Lim waveform reconstruction helper\n", "# ============================================================\n", "def mel_spec_to_waveform(mel_spec_logmel, sr=16000, n_fft=512, hop_length=256, n_mels=128):\n", "    \"\"\"\n", "    Reconstruct waveform from log1p-scaled mel spectrogram.\n", "    mel_spec_logmel: (n_mels, T) tensor\n", "    Returns: numpy array (samples,)\n", "    \"\"\"\n", "    mel_spec = torch.expm1(mel_spec_logmel.cpu().float()).clamp(min=0)\n", "\n", "    # Use 'gelsd' driver for robust least-squares inversion (SVD-based)\n", "    inv_mel = T.InverseMelScale(\n", "        n_stft=n_fft // 2 + 1,\n", "        n_mels=n_mels,\n", "        sample_rate=sr,\n", "        driver='gelsd'\n", "    )\n", "    linear_spec = inv_mel(mel_spec).clamp(min=0)  # (n_fft//2+1, T)\n", "\n", "    gl = T.GriffinLim(n_fft=n_fft, hop_length=hop_length, n_iter=32)\n", "    wav = gl(linear_spec)\n", "    return wav.numpy()\n", "\n", "\n", "def si_sdr(reference, estimate, eps=1e-8):\n", "    \"\"\"Scale-invariant SDR in dB (higher is better).\"\"\"\n", "    reference = reference - reference.mean()\n", "    estimate  = estimate  - estimate.mean()\n", "    alpha     = (estimate @ reference) / (reference @ reference + eps)\n", "    s_target  = alpha * reference\n", "    e_noise   = estimate - s_target\n", "    return 10 * np.log10((s_target ** 2).sum() / ((e_noise ** 2).sum() + eps))\n", "\n", "\n", "# Test with a smooth random spec (not raw randn which is ill-conditioned)\n", "print(\" Testing Griffin-Lim...\")\n", "_spec = torch.randn(128, 188).abs() + 0.1  # ensure non-zero, better conditioned\n", "_wav  = mel_spec_to_waveform(_spec)\n", "print(f\"  Spec: {_spec.shape}  ->  WAV: {_wav.shape}  ({_wav.shape[0]/16000:.2f}s)\")\n", "print(\" Griffin-Lim OK\")\n", "\n", "# Test SI-SDR\n", "_ref  = np.random.randn(16000).astype(np.float32)\n", "_est  = _ref * 0.9 + np.random.randn(16000).astype(np.float32) * 0.1\n", "_val  = si_sdr(_ref, _est)\n", "print(f\" SI-SDR test: {_val:.2f} dB  (should be >10 dB for clean signal)\")\n", "print(\" SI-SDR OK\")"]}, {"cell_type": "code", "execution_count": null, "id": "ed7218ac", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 11: Full evaluation \u2014 PESQ / STOI / SI-SDR\n", "# ============================================================\n", "def evaluate_model(model, dataset, num_samples=None, sr=16000):\n", "    if num_samples is None:\n", "        num_samples = len(dataset)\n", "    num_samples = min(num_samples, len(dataset))\n", "\n", "    results = {\n", "        'pesq_noisy': [], 'pesq_enhanced': [],\n", "        'stoi_noisy': [], 'stoi_enhanced': [],\n", "        'sisdr_noisy': [], 'sisdr_enhanced': []\n", "    }\n", "    errors = 0\n", "    model.eval()\n", "\n", "    with torch.no_grad():\n", "        for i in tqdm(range(num_samples), desc=\"Evaluating\"):\n", "            try:\n", "                sample       = dataset[i]\n", "                noisy_spec   = sample['noisy_spec'].unsqueeze(0).to(device)\n", "                enhanced_s, _ = model(noisy_spec)\n", "                enhanced_s   = enhanced_s.squeeze(0).cpu()\n", "\n", "                clean_wav    = sample['clean_wav'].cpu().numpy().astype(np.float32)\n", "                noisy_wav    = sample['noisy_wav'].cpu().numpy().astype(np.float32)\n", "                enhanced_wav = mel_spec_to_waveform(enhanced_s).astype(np.float32)\n", "\n", "                # Align lengths\n", "                L = min(len(clean_wav), len(noisy_wav), len(enhanced_wav))\n", "                clean_wav, noisy_wav, enhanced_wav = clean_wav[:L], noisy_wav[:L], enhanced_wav[:L]\n", "\n", "                # Normalise\n", "                for w in [clean_wav, noisy_wav, enhanced_wav]:\n", "                    w /= (np.abs(w).max() + 1e-8)\n", "\n", "                results['pesq_noisy'].append(pesq(sr, clean_wav, noisy_wav, 'wb'))\n", "                results['pesq_enhanced'].append(pesq(sr, clean_wav, enhanced_wav, 'wb'))\n", "                results['stoi_noisy'].append(stoi(clean_wav, noisy_wav, sr, extended=False))\n", "                results['stoi_enhanced'].append(stoi(clean_wav, enhanced_wav, sr, extended=False))\n", "                results['sisdr_noisy'].append(si_sdr(clean_wav, noisy_wav))\n", "                results['sisdr_enhanced'].append(si_sdr(clean_wav, enhanced_wav))\n", "\n", "            except Exception as e:\n", "                errors += 1\n", "\n", "    print(f\"  Completed: {num_samples - errors}/{num_samples}  ({errors} errors)\")\n", "    return results\n", "\n", "\n", "print(\" Loading best model checkpoint...\")\n", "ckpt  = torch.load(CHECKPOINT)\n", "model.load_state_dict(ckpt['model_state_dict'])\n", "print(f\"  Epoch {ckpt['epoch']}, val_loss={ckpt['val_loss']:.4f}\")\n", "\n", "print(f\"\\n Evaluating on {len(test_ds)} test samples (Griffin-Lim reconstruction)...\")\n", "results = evaluate_model(model, test_ds)"]}, {"cell_type": "code", "execution_count": null, "id": "d14c92b6", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 12: Evaluation summary & plots\n", "# ============================================================\n", "def print_metric(name, noisy_list, enhanced_list):\n", "    n, e = np.mean(noisy_list), np.mean(enhanced_list)\n", "    print(f\"  {name:14s}:  Noisy={n:.3f}  Enhanced={e:.3f}  Delta={e-n:+.3f}\")\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "print(\" EVALUATION RESULTS \u2014 CNN-Transformer (Review 2)\")\n", "print(\"=\"*60)\n", "if results['pesq_noisy']:    print_metric(\"PESQ (wb)\",   results['pesq_noisy'],  results['pesq_enhanced'])\n", "if results['stoi_noisy']:    print_metric(\"STOI\",        results['stoi_noisy'],  results['stoi_enhanced'])\n", "if results['sisdr_noisy']:   print_metric(\"SI-SDR (dB)\", results['sisdr_noisy'], results['sisdr_enhanced'])\n", "\n", "avg_pesq = np.mean(results['pesq_enhanced']) if results['pesq_enhanced'] else 0\n", "\n", "print(\"\\n\" + \"=\"*60)\n", "if   avg_pesq >= 3.2: print(f\" PESQ {avg_pesq:.3f} >= 3.2 \u2014 Review 2 target ACHIEVED!\")\n", "elif avg_pesq >= 3.0: print(f\" PESQ {avg_pesq:.3f} \u2014 Good, close to 3.2 target\")\n", "else:                 print(f\" PESQ {avg_pesq:.3f} \u2014 Needs tuning\")\n", "print(\"=\"*60)\n", "\n", "# Plots\n", "if results['pesq_noisy']:\n", "    metrics = [\n", "        ('PESQ', results['pesq_noisy'],  results['pesq_enhanced']),\n", "        ('STOI', results['stoi_noisy'],  results['stoi_enhanced']),\n", "        ('SI-SDR (dB)', results['sisdr_noisy'], results['sisdr_enhanced'])\n", "    ]\n", "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n", "    for ax, (name, noisy, enhanced) in zip(axes, metrics):\n", "        if not noisy: continue\n", "        ax.hist(noisy,    bins=20, alpha=0.6, color='red',   label='Noisy',    edgecolor='k')\n", "        ax.hist(enhanced, bins=20, alpha=0.6, color='green', label='Enhanced', edgecolor='k')\n", "        ax.axvline(np.mean(noisy),    color='darkred',   ls='--', lw=2, label=f'\u03bc={np.mean(noisy):.2f}')\n", "        ax.axvline(np.mean(enhanced), color='darkgreen', ls='--', lw=2, label=f'\u03bc={np.mean(enhanced):.2f}')\n", "        ax.set_title(f'{name} Distribution', fontweight='bold')\n", "        ax.set_xlabel(name); ax.set_ylabel('Count')\n", "        ax.legend(); ax.grid(alpha=0.3)\n", "    plt.tight_layout()\n", "    plt.savefig('transformer_eval_metrics.png', dpi=150, bbox_inches='tight')\n", "    plt.show()\n", "    print(\" Saved: transformer_eval_metrics.png\")"]}, {"cell_type": "markdown", "id": "773c7048", "metadata": {}, "source": ["## Attention Visualisation (Review 2 key deliverable)"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "488802af", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 13: Attention visualisation\n", "# ============================================================\n", "print(\" Visualising attention weights...\")\n", "\n", "# Use test_ds if available (Kaggle), else use synthetic data (local)\n", "if 'test_ds' in dir() or 'test_ds' in locals():\n", "    sample     = test_ds[0]\n", "    noisy_spec = sample['noisy_spec'].unsqueeze(0).to(device)\n", "else:\n", "    print(\"  (using synthetic spectrogram for local test)\")\n", "    noisy_spec = torch.randn(1, 128, 188).abs().to(device)\n", "\n", "with torch.no_grad():\n", "    attn_maps = get_attention_weights(model, noisy_spec)\n", "\n", "num_layers = len(attn_maps)\n", "nhead      = attn_maps[0].shape[1]\n", "fig, axes  = plt.subplots(num_layers, nhead, figsize=(5 * nhead, 4 * num_layers))\n", "\n", "if num_layers == 1:\n", "    axes = [axes]\n", "\n", "for layer_idx, attn in enumerate(attn_maps):\n", "    attn_np = attn[0].numpy()  # (nhead, T, T)\n", "    for head_idx in range(nhead):\n", "        ax = axes[layer_idx][head_idx]\n", "        im = ax.imshow(attn_np[head_idx], aspect='auto', origin='upper',\n", "                       cmap='hot', interpolation='nearest')\n", "        ax.set_title(f'Layer {layer_idx+1}, Head {head_idx+1}', fontsize=9)\n", "        ax.set_xlabel('Key (time)'); ax.set_ylabel('Query (time)')\n", "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n", "\n", "plt.suptitle('Transformer Self-Attention Weights\\n(CNN-Transformer Speech Enhancer \u2014 Review 2)',\n", "             fontsize=13, fontweight='bold')\n", "plt.tight_layout()\n", "plt.savefig('attention_weights.png', dpi=150, bbox_inches='tight')\n", "plt.show()\n", "print(\" Saved: attention_weights.png\")"]}, {"cell_type": "markdown", "id": "485fe84a", "metadata": {}, "source": ["## CRN vs Transformer \u2014 Review Comparison"], "outputs": [], "execution_count": null}, {"cell_type": "code", "execution_count": null, "id": "10c54048", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# CELL 14: CRN vs Transformer comparison summary\n", "# ============================================================\n", "crn_pesq   = 3.1\n", "crn_params = 2_000_000  # ~2M CRN\n", "\n", "transformer_pesq   = avg_pesq\n", "transformer_params = sum(p.numel() for p in model.parameters())\n", "\n", "print(\"\\n\" + \"=\"*68)\n", "print(\" COMPARISON: CRN Baseline (R1) vs CNN-Transformer (R2)\")\n", "print(\"=\"*68)\n", "print(f\"{'Metric':<26} {'CRN (R1)':>16} {'Transformer (R2)':>20}\")\n", "print(\"-\"*68)\n", "print(f\"{'PESQ (wideband)':<26} {crn_pesq:>16.2f} {transformer_pesq:>20.3f}\")\n", "print(f\"{'Parameters':<26} {crn_params:>16,} {transformer_params:>20,}\")\n", "print(f\"{'RNN/TF layers':<26} {'2 LSTM':>16} {'2 TF (4H, Pre-LN)':>20}\")\n", "print(f\"{'Architecture':<26} {'CNN+LSTM':>16} {'CNN+Transformer':>20}\")\n", "print(\"-\"*68)\n", "\n", "param_reduction = (1 - transformer_params / crn_params) * 100\n", "pesq_delta      = transformer_pesq - crn_pesq\n", "\n", "print(f\"\\n Param change : {param_reduction:+.1f}%  ({transformer_params:,} vs {crn_params:,})\")\n", "print(f\" PESQ change  : {pesq_delta:+.3f}  ({transformer_pesq:.3f} vs {crn_pesq:.2f})\")\n", "\n", "if transformer_pesq >= 3.2 and transformer_params < crn_params:\n", "    print(\"\\n Both goals met: PESQ \u2265 3.2 AND fewer params than CRN!\")\n", "elif transformer_pesq >= 3.2:\n", "    print(\"\\n PESQ target met (\u2265 3.2)!\")\n", "\n", "print(\"=\"*68)\n", "\n", "# Save summary\n", "summary = {\n", "    'review': 2,\n", "    'pesq_crn': crn_pesq, 'pesq_transformer': transformer_pesq,\n", "    'params_crn': crn_params, 'params_transformer': transformer_params,\n", "    'param_reduction_pct': param_reduction,\n", "    'pesq_improvement': pesq_delta,\n", "    'best_epoch': history['best_epoch'],\n", "    'best_val_loss': best_val_loss,\n", "}\n", "import json\n", "with open('review2_summary.json', 'w') as f:\n", "    json.dump(summary, f, indent=2)\n", "print(\"\\n Saved: review2_summary.json\")"]}, {"cell_type": "code", "execution_count": null, "id": "84290c3c", "metadata": {}, "outputs": [], "source": ["# ============================================================\n", "# SMOKE TEST: Mini training loop on synthetic data (LOCAL VALIDATION)\n", "# Verifies grads, loss decreasing, mask in [0,1], shapes all correct.\n", "# Remove/skip this cell on Kaggle.\n", "# ============================================================\n", "print(\" SMOKE TEST: mini training loop on synthetic data\")\n", "print(\"=\"*55)\n", "\n", "# Build a fresh small model for quick test\n", "_model_test = ShallowTransformerEnhancer(n_mels=128, d_model=64, nhead=4,\n", "                                          num_layers=2, ff_dim=256, dropout=0.0).to(device)\n", "_opt   = optim.Adam(_model_test.parameters(), lr=1e-3)\n", "_crit  = nn.L1Loss()\n", "_model_test.train()\n", "\n", "losses = []\n", "for step in range(10):\n", "    # Synthetic batch: 4 samples, 128 mels, 188 time steps\n", "    _noisy = torch.randn(4, 128, 188).abs().to(device)\n", "    _clean = (_noisy * 0.7 + torch.randn_like(_noisy).abs() * 0.1)  # slightly different clean\n", "\n", "    _opt.zero_grad()\n", "    _enhanced, _mask = _model_test(_noisy)\n", "\n", "    # Verify output properties\n", "    assert _enhanced.shape == (4, 128, 188), f\"Bad shape: {_enhanced.shape}\"\n", "    assert _mask.min() >= 0.0 and _mask.max() <= 1.0, \"Mask out of [0,1]!\"\n", "\n", "    _loss = _crit(_enhanced, _clean)\n", "    _loss.backward()\n", "    _opt.step()\n", "    losses.append(_loss.item())\n", "    print(f\"  Step {step+1:2d}: loss={_loss.item():.4f}\")\n", "\n", "print(f\"\\n  Loss trend: {losses[0]:.4f} -> {losses[-1]:.4f}\")\n", "improved = losses[-1] < losses[0]\n", "print(f\"  Loss decreasing: {'YES' if improved else 'NO (may be OK for 10 steps)'}\")\n", "print(f\"\\n SMOKE TEST PASSED\")\n", "print(\"=\"*55)\n", "del _model_test, _opt"]}], "metadata": {"kernelspec": {"display_name": ".venv (3.13.1)", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.13.1"}}, "nbformat": 4, "nbformat_minor": 5}