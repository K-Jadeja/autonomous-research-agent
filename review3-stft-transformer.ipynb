{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31d89cbc",
   "metadata": {},
   "source": [
    "# Review 3: STFT-Based CNN-Transformer Speech Enhancement\n",
    "\n",
    "**Key Fix from Review 2:** Replaced mel spectrogram (non-invertible) with STFT (phase-preserving).  \n",
    "**Architecture:** CNN encoder (1\u219264\u2192128\u2192256) + 2-layer Pre-LN Transformer (4 heads, d=256) + CNN decoder \u2192 Sigmoid mask  \n",
    "**Reconstruction:** `mask \u00d7 complex_STFT \u2192 ISTFT` (lossless \u2014 no GriffinLim needed)  \n",
    "\n",
    "**Why R2 failed:** MelSpectrogram is a many-to-one mapping. `InverseMelScale + GriffinLim` introduced  \n",
    "catastrophic phase artifacts (SI-SDR went from \u22120.82 dB to \u221225.58 dB). The model WAS learning  \n",
    "(val loss improved from 0.1764\u21920.1485) but reconstruction destroyed the signal.\n",
    "\n",
    "**Team:** Krishnasinh Jadeja (22BLC1211), Kirtan Sondagar (22BLC1228), Prabhu Kalyan Panda (22BLC1213)  \n",
    "**Guide:** Dr. Praveen Jaraut \u2014 VIT Bhopal Capstone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c5c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 1: Install dependencies + Imports + Device check\n",
    "# ============================================================================\n",
    "!pip install pesq==0.0.4 pystoi -q\n",
    "\n",
    "import torch, torch.nn as nn, torch.nn.functional as F\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import glob, os, json, time, warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from pesq import pesq as pesq_metric\n",
    "from pystoi import stoi as stoi_metric\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# STFT Configuration (matches CRN baseline)\n",
    "N_FFT = 512\n",
    "HOP_LENGTH = 256\n",
    "N_FREQ = N_FFT // 2 + 1  # 257 frequency bins\n",
    "SR = 16000\n",
    "MAX_LEN = 48000  # 3 seconds at 16kHz\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {device}')\n",
    "if device == 'cuda':\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    vram = getattr(props, 'total_memory', getattr(props, 'total_mem', 0))\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)} | VRAM: {vram / 1e9:.1f}GB')\n",
    "print(f'STFT config: n_fft={N_FFT}, hop={HOP_LENGTH}, freq_bins={N_FREQ}')\n",
    "print('Imports OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff528d9",
   "metadata": {},
   "source": [
    "## Dataset: LibriSpeech-Noise\n",
    "Download and extract `earth16/libri-speech-noise-dataset` (6.6 GB, 7000 train + 105 test WAV pairs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c480dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 2: Dataset download & extraction (Kaggle)\n",
    "# ============================================================================\n",
    "import subprocess, zipfile\n",
    "\n",
    "data_base = '/kaggle/working/data'\n",
    "dl_tmp = '/kaggle/working/dl_tmp'\n",
    "os.makedirs(data_base, exist_ok=True)\n",
    "os.makedirs(dl_tmp, exist_ok=True)\n",
    "done_flag = os.path.join(data_base, '.done')\n",
    "\n",
    "if os.path.exists(done_flag):\n",
    "    print('Dataset already extracted, skipping')\n",
    "else:\n",
    "    mounted = '/kaggle/input/libri-speech-noise-dataset'\n",
    "    if os.path.isdir(mounted) and len(os.listdir(mounted)) > 0:\n",
    "        src = mounted\n",
    "        print(f'Using mounted dataset at {src}')\n",
    "    else:\n",
    "        print('Dataset not mounted, downloading via kaggle API...')\n",
    "        subprocess.run(['kaggle', 'datasets', 'download',\n",
    "                        'earth16/libri-speech-noise-dataset', '-p', dl_tmp], check=True)\n",
    "        zf = os.path.join(dl_tmp, 'libri-speech-noise-dataset.zip')\n",
    "        if os.path.exists(zf):\n",
    "            with zipfile.ZipFile(zf, 'r') as z:\n",
    "                z.extractall(dl_tmp)\n",
    "            os.remove(zf)\n",
    "        src = dl_tmp\n",
    "        print(f'Downloaded to {src}')\n",
    "\n",
    "    # Extract 7z archives\n",
    "    subprocess.run(['apt-get', 'install', '-y', 'p7zip-full'], capture_output=True)\n",
    "    for arch in ['train.7z', 'y_train.7z', 'test.7z', 'y_test.7z']:\n",
    "        fp = os.path.join(src, arch)\n",
    "        if os.path.exists(fp):\n",
    "            print(f'Extracting {arch}...')\n",
    "            subprocess.run(['7z', 'x', fp, f'-o{data_base}', '-y'], capture_output=True)\n",
    "    open(done_flag, 'w').close()\n",
    "\n",
    "# Verify extraction\n",
    "def find_wav_dir(base, name):\n",
    "    for root, dirs, files in os.walk(base):\n",
    "        if os.path.basename(root) == name and any(f.endswith('.wav') for f in files):\n",
    "            return root\n",
    "    return None\n",
    "\n",
    "noisy_train = find_wav_dir(data_base, 'train')\n",
    "clean_train = find_wav_dir(data_base, 'y_train')\n",
    "noisy_test  = find_wav_dir(data_base, 'test')\n",
    "clean_test  = find_wav_dir(data_base, 'y_test')\n",
    "\n",
    "for tag, d in [('noisy_train', noisy_train), ('clean_train', clean_train),\n",
    "               ('noisy_test', noisy_test), ('clean_test', clean_test)]:\n",
    "    n = len(glob.glob(os.path.join(d, '*.wav'))) if d else 0\n",
    "    print(f'  {tag}: {d} ({n} files)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907e8cec",
   "metadata": {},
   "source": [
    "## STFT Dataset Class\n",
    "Loads WAV pairs \u2192 computes STFT (n_fft=512, hop=256) \u2192 returns magnitude, phase, and waveforms.  \n",
    "**Key difference from R2:** No mel filterbank. STFT is losslessly invertible via ISTFT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fae2c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 3: STFTSpeechDataset\n",
    "# ============================================================================\n",
    "class STFTSpeechDataset(Dataset):\n",
    "    \"\"\"STFT-based speech enhancement dataset. Returns magnitude, phase, and waveforms.\"\"\"\n",
    "    def __init__(self, noisy_dir, clean_dir, n_fft=N_FFT, hop_length=HOP_LENGTH,\n",
    "                 sr=SR, max_len=MAX_LEN):\n",
    "        self.noisy_files = sorted(glob.glob(os.path.join(noisy_dir, '*.wav')))\n",
    "        self.clean_files = sorted(glob.glob(os.path.join(clean_dir, '*.wav')))\n",
    "        assert len(self.noisy_files) == len(self.clean_files), \\\n",
    "            f'Mismatch: {len(self.noisy_files)} noisy vs {len(self.clean_files)} clean'\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.sr = sr\n",
    "        self.max_len = max_len\n",
    "        self.window = torch.hann_window(n_fft)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.noisy_files)\n",
    "\n",
    "    def _load_fix(self, path):\n",
    "        wav, sr = torchaudio.load(path)\n",
    "        if sr != self.sr:\n",
    "            wav = torchaudio.functional.resample(wav, sr, self.sr)\n",
    "        wav = wav[0]  # mono, shape (samples,)\n",
    "        if wav.shape[0] > self.max_len:\n",
    "            start = torch.randint(0, wav.shape[0] - self.max_len, (1,)).item()\n",
    "            wav = wav[start:start + self.max_len]\n",
    "        elif wav.shape[0] < self.max_len:\n",
    "            wav = F.pad(wav, (0, self.max_len - wav.shape[0]))\n",
    "        return wav\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        noisy_wav = self._load_fix(self.noisy_files[idx])\n",
    "        clean_wav = self._load_fix(self.clean_files[idx])\n",
    "\n",
    "        noisy_stft = torch.stft(noisy_wav, self.n_fft, self.hop_length,\n",
    "                                window=self.window, return_complex=True)\n",
    "        clean_stft = torch.stft(clean_wav, self.n_fft, self.hop_length,\n",
    "                                window=self.window, return_complex=True)\n",
    "\n",
    "        return {\n",
    "            'noisy_mag':   noisy_stft.abs(),          # (257, T)\n",
    "            'clean_mag':   clean_stft.abs(),          # (257, T)\n",
    "            'noisy_phase': torch.angle(noisy_stft),   # (257, T)\n",
    "            'noisy_wav':   noisy_wav,                 # (max_len,)\n",
    "            'clean_wav':   clean_wav,                 # (max_len,)\n",
    "        }\n",
    "\n",
    "print(f'STFTSpeechDataset defined \u2014 n_fft={N_FFT}, hop={HOP_LENGTH}, freq_bins={N_FREQ}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9772e31c",
   "metadata": {},
   "source": [
    "## Model: STFTTransformerEnhancer\n",
    "\n",
    "**Same architecture as Review 2**, only the input dimension changes (257 STFT bins instead of 128 mel bins).\n",
    "\n",
    "```\n",
    "Input: (B, 1, 257, T) \u2190 log1p(STFT magnitude)\n",
    "  \u2192 CNN Encoder: Conv2d 1\u219264\u2192128\u2192256 (3\u00d73, BN, ReLU)\n",
    "  \u2192 mean(freq dim) \u2192 (B, T, 256) \u2192 Linear(256\u2192256)\n",
    "  \u2192 Sinusoidal PositionalEncoding\n",
    "  \u2192 2-layer Pre-LN TransformerEncoder (4 heads, d=256, ff=1024)\n",
    "  \u2192 Linear(256\u2192256) \u2192 expand to (B, 256, 257, T)\n",
    "  \u2192 CNN Decoder: Conv2d 256\u2192128\u219264\u21921 + Sigmoid\n",
    "Output: mask (B, 257, T) in [0, 1]\n",
    "Enhanced = mask \u00d7 noisy_magnitude \u2192 ISTFT with noisy phase \u2192 waveform\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789d3d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 4: Model definition + forward pass test\n",
    "# ============================================================================\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_c, out_c, k=3, s=1, p=1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv2d(in_c, out_c, k, s, p),\n",
    "            nn.BatchNorm2d(out_c),\n",
    "            nn.ReLU(inplace=True))\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=2000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(max_len).unsqueeze(1).float()\n",
    "        div = torch.exp(torch.arange(0, d_model, 2).float() * (-np.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "    def forward(self, x):\n",
    "        return self.dropout(x + self.pe[:, :x.size(1)])\n",
    "\n",
    "class STFTTransformerEnhancer(nn.Module):\n",
    "    def __init__(self, n_freq=257, d_model=256, nhead=4, num_layers=2,\n",
    "                 dim_ff=1024, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_freq = n_freq\n",
    "        # CNN Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            ConvBlock(1, 64), ConvBlock(64, 128), ConvBlock(128, 256))\n",
    "        # Transformer\n",
    "        self.pre_proj  = nn.Linear(256, d_model)\n",
    "        self.pos_enc   = PositionalEncoding(d_model, dropout)\n",
    "        enc_layer = nn.TransformerEncoderLayer(\n",
    "            d_model, nhead, dim_ff, dropout, batch_first=True, norm_first=True)\n",
    "        self.transformer = nn.TransformerEncoder(enc_layer, num_layers)\n",
    "        self.post_proj = nn.Linear(d_model, 256)\n",
    "        # CNN Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            ConvBlock(256, 128), ConvBlock(128, 64),\n",
    "            nn.Conv2d(64, 1, 3, 1, 1), nn.Sigmoid())\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
    "                nn.init.kaiming_normal_(m.weight, nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, 1, n_freq, T)\n",
    "        enc = self.encoder(x)                                    # (B, 256, n_freq, T)\n",
    "        feat = enc.mean(dim=2).permute(0, 2, 1)                 # (B, T, 256)\n",
    "        feat = self.pos_enc(self.pre_proj(feat))                 # (B, T, d_model)\n",
    "        feat = self.post_proj(self.transformer(feat))            # (B, T, 256)\n",
    "        feat = feat.permute(0, 2, 1)                             # (B, 256, T)\n",
    "        feat = feat.unsqueeze(2).expand(-1, -1, self.n_freq, -1) # (B, 256, n_freq, T)\n",
    "        mask = self.decoder(feat).squeeze(1)                     # (B, n_freq, T)\n",
    "        return mask\n",
    "\n",
    "# ---- Test ----\n",
    "model = STFTTransformerEnhancer(n_freq=N_FREQ).to(device)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'Params: {total_params:,} ({total_params/1e6:.2f}M)')\n",
    "\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(2, 1, N_FREQ, 188).to(device)\n",
    "    out = model(dummy)\n",
    "    print(f'Input: {dummy.shape} \u2192 Mask: {out.shape}')\n",
    "    assert out.shape == (2, N_FREQ, 188), f'Shape mismatch: {out.shape}'\n",
    "    assert 0 <= out.min() and out.max() <= 1, 'Mask not in [0,1]'\n",
    "    print('Forward pass OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cf9aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 5: Attention weight extraction + SI-SDR utility\n",
    "# ============================================================================\n",
    "def get_attention_weights(mdl, x):\n",
    "    \"\"\"Extract self-attention weights by manual Pre-LN transformer forward.\"\"\"\n",
    "    mdl.eval()\n",
    "    weights = []\n",
    "    with torch.no_grad():\n",
    "        enc = mdl.encoder(x)\n",
    "        feat = enc.mean(dim=2).permute(0, 2, 1)\n",
    "        feat = mdl.pos_enc(mdl.pre_proj(feat))\n",
    "        for layer in mdl.transformer.layers:\n",
    "            normed = layer.norm1(feat)\n",
    "            attn_out, w = layer.self_attn(\n",
    "                normed, normed, normed,\n",
    "                need_weights=True, average_attn_weights=False)\n",
    "            weights.append(w.cpu())  # (B, nhead, T, T)\n",
    "            feat = feat + layer.dropout1(attn_out)\n",
    "            # Feedforward: replicate Pre-LN FF block\n",
    "            normed2 = layer.norm2(feat)\n",
    "            ff_out = layer.linear2(\n",
    "                F.dropout(layer.activation(layer.linear1(normed2)),\n",
    "                          p=0.0, training=False))\n",
    "            feat = feat + ff_out\n",
    "    return weights\n",
    "\n",
    "def si_sdr(estimate, reference):\n",
    "    \"\"\"Scale-Invariant Signal-to-Distortion Ratio (dB).\"\"\"\n",
    "    ref = reference - reference.mean()\n",
    "    est = estimate - estimate.mean()\n",
    "    dot = torch.sum(ref * est)\n",
    "    s_target = dot * ref / (torch.sum(ref ** 2) + 1e-8)\n",
    "    e_noise = est - s_target\n",
    "    return 10 * torch.log10(torch.sum(s_target**2) / (torch.sum(e_noise**2) + 1e-8) + 1e-8)\n",
    "\n",
    "# Test attention extraction\n",
    "with torch.no_grad():\n",
    "    dummy = torch.randn(1, 1, N_FREQ, 188).to(device)\n",
    "    attn = get_attention_weights(model, dummy)\n",
    "    print(f'Attention: {len(attn)} layers, shape: {attn[0].shape}')\n",
    "print('Attention extraction OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae0602",
   "metadata": {},
   "source": [
    "## Training\n",
    "\n",
    "**Config:** L1 loss on log-magnitudes, Adam lr=1e-3, ReduceLROnPlateau, 25 epochs, early stopping patience=10.  \n",
    "**Same hyperparameters as Review 2** \u2014 only the spectrogram pipeline changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0971007e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 6: Training setup \u2014 dataloaders, optimizer, scheduler\n",
    "# ============================================================================\n",
    "MAX_EPOCHS = 25\n",
    "LR = 1e-3\n",
    "BATCH = 16\n",
    "PATIENCE = 10\n",
    "CKPT = 'stft_transformer_best.pth'\n",
    "\n",
    "# Re-init model fresh for training\n",
    "model = STFTTransformerEnhancer(n_freq=N_FREQ).to(device)\n",
    "print(f'Params: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')\n",
    "print('Fresh training, Kaiming init')\n",
    "\n",
    "# Datasets\n",
    "full_train = STFTSpeechDataset(noisy_train, clean_train)\n",
    "n_val = int(0.1 * len(full_train))\n",
    "n_train = len(full_train) - n_val\n",
    "train_ds, val_ds = torch.utils.data.random_split(\n",
    "    full_train, [n_train, n_val], generator=torch.Generator().manual_seed(42))\n",
    "test_ds = STFTSpeechDataset(noisy_test, clean_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,\n",
    "                          num_workers=0, drop_last=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=0)\n",
    "\n",
    "# Optimizer & Scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, 'min', factor=0.5, patience=5)\n",
    "\n",
    "print(f'train: {n_train} samples')\n",
    "print(f'val: {n_val} samples')\n",
    "print(f'Train:{n_train} Val:{n_val} Test:{len(test_ds)} | BS={BATCH} LR={LR}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485c6438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 7: Training loop\n",
    "# ============================================================================\n",
    "history = {'train_loss': [], 'val_loss': []}\n",
    "best_val = float('inf')\n",
    "patience_ctr = 0\n",
    "t0 = time.time()\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for batch in tqdm(train_loader, desc=f'Ep{epoch}/{MAX_EPOCHS}', leave=False):\n",
    "        noisy_mag = batch['noisy_mag'].to(device)   # (B, 257, T)\n",
    "        clean_mag = batch['clean_mag'].to(device)   # (B, 257, T)\n",
    "\n",
    "        inp = torch.log1p(noisy_mag).unsqueeze(1)   # (B, 1, 257, T)\n",
    "        mask = model(inp)                            # (B, 257, T)\n",
    "        enhanced_mag = mask * noisy_mag\n",
    "\n",
    "        loss = F.l1_loss(torch.log1p(enhanced_mag), torch.log1p(clean_mag))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    # ---- Validate ----\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            noisy_mag = batch['noisy_mag'].to(device)\n",
    "            clean_mag = batch['clean_mag'].to(device)\n",
    "            inp = torch.log1p(noisy_mag).unsqueeze(1)\n",
    "            mask = model(inp)\n",
    "            enhanced_mag = mask * noisy_mag\n",
    "            loss = F.l1_loss(torch.log1p(enhanced_mag), torch.log1p(clean_mag))\n",
    "            val_losses.append(loss.item())\n",
    "\n",
    "    tr_loss = np.mean(train_losses)\n",
    "    va_loss = np.mean(val_losses)\n",
    "    history['train_loss'].append(tr_loss)\n",
    "    history['val_loss'].append(va_loss)\n",
    "    scheduler.step(va_loss)\n",
    "\n",
    "    elapsed = time.time() - t0\n",
    "    lr_now = optimizer.param_groups[0]['lr']\n",
    "    line = f'Ep{epoch:02d} tr={tr_loss:.4f} va={va_loss:.4f} lr={lr_now:.1e} [{elapsed:.0f}s]'\n",
    "\n",
    "    if va_loss < best_val:\n",
    "        best_val = va_loss\n",
    "        patience_ctr = 0\n",
    "        torch.save({'epoch': epoch, 'model': model.state_dict(), 'val_loss': float(va_loss)}, CKPT)\n",
    "        print(f'{line}  SAVED best val={va_loss:.4f}')\n",
    "    else:\n",
    "        patience_ctr += 1\n",
    "        print(f'{line}  no improve ({patience_ctr}/{PATIENCE})')\n",
    "\n",
    "    if epoch % 5 == 0:\n",
    "        torch.save(model.state_dict(), f'ckpt_ep{epoch}.pth')\n",
    "\n",
    "    if patience_ctr >= PATIENCE:\n",
    "        print(f'Early stopping at epoch {epoch}')\n",
    "        break\n",
    "\n",
    "best_ep = history['val_loss'].index(min(history['val_loss'])) + 1\n",
    "print(f'\\nDONE best_ep={best_ep} best_val={best_val:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ac5870",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3825875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 8: Training curves\n",
    "# ============================================================================\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 5))\n",
    "epochs_range = range(1, len(history['train_loss']) + 1)\n",
    "ax.plot(epochs_range, history['train_loss'], 'b-o', label='Train Loss', markersize=4)\n",
    "ax.plot(epochs_range, history['val_loss'], 'r-s', label='Val Loss', markersize=4)\n",
    "ax.axvline(x=best_ep, color='green', linestyle='--', alpha=0.7, label=f'Best epoch ({best_ep})')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('L1 Loss (log-magnitude)')\n",
    "ax.set_title('Review 3: STFT-Transformer Training Curves')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved training_curves.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffcfaeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 9: Evaluation \u2014 PESQ / STOI / SI-SDR with ISTFT waveform reconstruction\n",
    "# ============================================================================\n",
    "# Load best model\n",
    "ckpt = torch.load(CKPT, map_location=device, weights_only=False)\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.eval()\n",
    "print(f'Loaded ep {ckpt[\"epoch\"]}, val={ckpt[\"val_loss\"]:.4f}')\n",
    "\n",
    "window = torch.hann_window(N_FFT).to(device)\n",
    "\n",
    "pesq_noisy_list, pesq_enh_list = [], []\n",
    "stoi_noisy_list, stoi_enh_list = [], []\n",
    "sisdr_noisy_list, sisdr_enh_list = [], []\n",
    "\n",
    "for i in tqdm(range(len(test_ds)), desc='Eval'):\n",
    "    sample = test_ds[i]\n",
    "    noisy_mag   = sample['noisy_mag'].unsqueeze(0).to(device)    # (1, 257, T)\n",
    "    noisy_phase = sample['noisy_phase'].unsqueeze(0).to(device)  # (1, 257, T)\n",
    "    clean_wav_np = sample['clean_wav'].numpy()\n",
    "    noisy_wav_np = sample['noisy_wav'].numpy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inp  = torch.log1p(noisy_mag).unsqueeze(1)   # (1, 1, 257, T)\n",
    "        mask = model(inp)                             # (1, 257, T)\n",
    "        enhanced_mag = (mask * noisy_mag).squeeze(0)  # (257, T)\n",
    "\n",
    "    # Reconstruct waveform: magnitude \u00d7 exp(j\u00b7phase) \u2192 ISTFT\n",
    "    enhanced_stft = enhanced_mag * torch.exp(1j * noisy_phase.squeeze(0))\n",
    "    enhanced_wav = torch.istft(enhanced_stft, N_FFT, HOP_LENGTH,\n",
    "                               window=window, length=MAX_LEN)\n",
    "    enh_np = enhanced_wav.cpu().numpy()\n",
    "\n",
    "    # PESQ\n",
    "    try:\n",
    "        pesq_noisy_list.append(pesq_metric(SR, clean_wav_np, noisy_wav_np, 'wb'))\n",
    "        pesq_enh_list.append(pesq_metric(SR, clean_wav_np, enh_np, 'wb'))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # STOI\n",
    "    stoi_noisy_list.append(stoi_metric(clean_wav_np, noisy_wav_np, SR, extended=False))\n",
    "    stoi_enh_list.append(stoi_metric(clean_wav_np, enh_np, SR, extended=False))\n",
    "\n",
    "    # SI-SDR\n",
    "    c_t = torch.from_numpy(clean_wav_np).float()\n",
    "    n_t = torch.from_numpy(noisy_wav_np).float()\n",
    "    e_t = torch.from_numpy(enh_np).float()\n",
    "    sisdr_noisy_list.append(si_sdr(n_t, c_t).item())\n",
    "    sisdr_enh_list.append(si_sdr(e_t, c_t).item())\n",
    "\n",
    "# Aggregate\n",
    "avg_pesq_n = np.mean(pesq_noisy_list) if pesq_noisy_list else 0.0\n",
    "avg_pesq_e = np.mean(pesq_enh_list) if pesq_enh_list else 0.0\n",
    "avg_stoi_n = np.mean(stoi_noisy_list)\n",
    "avg_stoi_e = np.mean(stoi_enh_list)\n",
    "avg_sisdr_n = np.mean(sisdr_noisy_list)\n",
    "avg_sisdr_e = np.mean(sisdr_enh_list)\n",
    "\n",
    "print(f'\\nPESQ: noisy={avg_pesq_n:.3f}  enhanced={avg_pesq_e:.3f}')\n",
    "print(f'STOI: {avg_stoi_n:.3f} -> {avg_stoi_e:.3f}')\n",
    "print(f'SI-SDR: {avg_sisdr_n:.2f}dB -> {avg_sisdr_e:.2f}dB')\n",
    "print(f'\\n--- Comparison with previous reviews ---')\n",
    "print(f'R1 CRN (estimated, no waveform recon): PESQ~3.10')\n",
    "print(f'R2 Transformer+Mel (real, GriffinLim):  PESQ={1.141:.3f}')\n",
    "print(f'R3 Transformer+STFT (real, ISTFT):      PESQ={avg_pesq_e:.3f}  delta={avg_pesq_e - 1.141:+.3f}')\n",
    "print(f'\\nParams: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afac9888",
   "metadata": {},
   "source": [
    "## Analysis: Attention Visualization & Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ad79bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 10: Attention visualization\n",
    "# ============================================================================\n",
    "# Use first test sample\n",
    "sample = test_ds[0]\n",
    "inp = torch.log1p(sample['noisy_mag'].unsqueeze(0).unsqueeze(0)).to(device)  # (1,1,257,T)\n",
    "attn_weights = get_attention_weights(model, inp)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 8))\n",
    "for layer_idx, aw in enumerate(attn_weights):\n",
    "    for head_idx in range(4):\n",
    "        ax = axes[layer_idx, head_idx]\n",
    "        w = aw[0, head_idx].numpy()  # (T, T)\n",
    "        ax.imshow(w[:64, :64], aspect='auto', cmap='viridis')\n",
    "        ax.set_title(f'L{layer_idx+1} H{head_idx+1}', fontsize=11)\n",
    "        ax.set_xlabel('Key')\n",
    "        ax.set_ylabel('Query')\n",
    "plt.suptitle('Self-Attention Weights (first 64 frames)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('attention_weights.png', dpi=150)\n",
    "plt.show()\n",
    "print('Saved attention_weights.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebdc3588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# Cell 11: Save review3_summary.json + final comparison\n",
    "# ============================================================================\n",
    "summary = {\n",
    "    'review': 3,\n",
    "    'model': 'STFTTransformerEnhancer',\n",
    "    'pipeline': 'STFT (n_fft=512, hop=256) \u2192 mask \u2192 ISTFT',\n",
    "    'params': sum(p.numel() for p in model.parameters() if p.requires_grad),\n",
    "    'best_epoch': int(ckpt['epoch']),\n",
    "    'best_val_loss': float(ckpt['val_loss']),\n",
    "    'epochs_trained': len(history['train_loss']),\n",
    "    'metrics': {\n",
    "        'pesq_noisy': round(avg_pesq_n, 3),\n",
    "        'pesq_enhanced': round(avg_pesq_e, 3),\n",
    "        'stoi_noisy': round(avg_stoi_n, 4),\n",
    "        'stoi_enhanced': round(avg_stoi_e, 4),\n",
    "        'sisdr_noisy_dB': round(avg_sisdr_n, 2),\n",
    "        'sisdr_enhanced_dB': round(avg_sisdr_e, 2),\n",
    "    },\n",
    "    'comparison': {\n",
    "        'R1_CRN_pesq_estimated': 3.10,\n",
    "        'R2_Transformer_Mel_pesq_real': 1.141,\n",
    "        'R3_Transformer_STFT_pesq_real': round(avg_pesq_e, 3),\n",
    "    },\n",
    "    'history': history,\n",
    "}\n",
    "\n",
    "with open('review3_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "print('Saved: review3_summary.json')\n",
    "\n",
    "# ---- Pretty comparison table ----\n",
    "print('\\n' + '='*70)\n",
    "print(f'{\"Review\":>10} {\"Pipeline\":>25} {\"PESQ\":>8} {\"STOI\":>8} {\"SI-SDR\":>10} {\"Params\":>10}')\n",
    "print('='*70)\n",
    "print(f'{\"R1 CRN\":>10} {\"Mel (estimated)\":>25} {\"~3.10\":>8} {\"\u2014\":>8} {\"\u2014\":>10} {\"~2.5M\":>10}')\n",
    "print(f'{\"R2 Trans\":>10} {\"Mel+GriffinLim\":>25} {1.141:>8.3f} {0.695:>8.3f} {-25.58:>9.2f}dB {\"2.45M\":>10}')\n",
    "print(f'{\"R3 Trans\":>10} {\"STFT+ISTFT\":>25} {avg_pesq_e:>8.3f} {avg_stoi_e:>8.3f} {avg_sisdr_e:>9.2f}dB {total_params/1e6:>8.2f}M')\n",
    "print('='*70)\n",
    "print(f'\\nNoisy baseline: PESQ={avg_pesq_n:.3f}  STOI={avg_stoi_n:.3f}  SI-SDR={avg_sisdr_n:.2f}dB')\n",
    "print(f'R3 improvement: dPESQ={avg_pesq_e - avg_pesq_n:+.3f}  dSTOI={avg_stoi_e - avg_stoi_n:+.4f}  dSI-SDR={avg_sisdr_e - avg_sisdr_n:+.2f}dB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}